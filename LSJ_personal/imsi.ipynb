{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 입력 텐서 정의\n",
    "input_tensor = torch.tensor([[[1, 1, 1]], [[0, 0, 0]]], dtype=torch.float32)\n",
    "\n",
    "# 텐서를 FloatTensor로 변환하고 배치 차원 추가\n",
    "con = nn.Conv1d(in_channels=1, out_channels=5, kernel_size=1,\n",
    "                              stride=1, dilation=1, padding=0, bias=False)\n",
    "\n",
    "# 출력 텐서 확인\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchlibrosa.stft import STFT, ISTFT, magphase\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2048\n",
    "hop_size = config.hop_samples\n",
    "window = 'hann'\n",
    "pad_mode = 'reflect'\n",
    "center = True\n",
    "momentum = 0.01\n",
    "downsample_ratio = 2**6\n",
    "channels=2\n",
    "activation='relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stft = STFT(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, \n",
    "            pad_mode=pad_mode, freeze_parameters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav_to_spectrogram(input):\n",
    "    \"\"\"Waveform to spectrogram.\n",
    "\n",
    "    Args:\n",
    "        input: (batch_size, segment_samples, channels_num)\n",
    "\n",
    "    Outputs:\n",
    "        output: (batch_size, channels_num, time_steps, freq_bins)\n",
    "    \"\"\"\n",
    "    sp_list = []\n",
    "    #####\n",
    "    channels_num = input.shape[2]\n",
    "    for channel in range(channels_num):\n",
    "        sp_list.append(spectrogram(input[:, :, channel]))\n",
    "    #####\n",
    "    # for _ in range(self.channels):\n",
    "    #     sp_list.append(self.spectrogram(input[:,:,0]))\n",
    "\n",
    "    output = torch.cat(sp_list, dim=1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectrogram(input):\n",
    "    (real, imag) = stft(input)\n",
    "    return (real ** 2 + imag ** 2) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn0 = nn.BatchNorm2d(window_size // 2 + 1, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_block1 = EncoderBlock(in_channels=channels, out_channels=32, \n",
    "            downsample=(2, 2), activation=activation, momentum=momentum, classes_num = config.latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    " \n",
    "    if hasattr(layer, 'bias'):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_emb(layer):\n",
    "    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n",
    "    nn.init.uniform_(layer.weight, -0.1, 0.1)\n",
    " \n",
    "    if hasattr(layer, 'bias'):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "def act(x, activation):\n",
    "    if activation == 'relu':\n",
    "        return F.relu_(x)\n",
    "\n",
    "    elif activation == 'leaky_relu':\n",
    "        return F.leaky_relu_(x, negative_slope=0.2)\n",
    "\n",
    "    elif activation == 'swish':\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "    else:\n",
    "        raise Exception('Incorrect activation!')\n",
    "\n",
    "def init_bn(bn):\n",
    "    \"\"\"Initialize a Batchnorm layer. \"\"\"\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, size, activation, momentum, classes_num = 527):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        pad = size // 2\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(size, size), stride=(1, 1), \n",
    "                              dilation=(1, 1), padding=(pad, pad), bias=False)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels, momentum=momentum)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(size, size), stride=(1, 1), \n",
    "                              dilation=(1, 1), padding=(pad, pad), bias=False)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels, momentum=momentum)\n",
    "        # change autotagging size\n",
    "        #####\n",
    "        self.emb1 = nn.Linear(classes_num, out_channels, bias=True)\n",
    "        self.emb2 = nn.Linear(classes_num, out_channels, bias=True)\n",
    "        ####\n",
    "        # self.emb1 = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=size,\n",
    "        #                       stride=1, dilation=1, padding=pad, bias=False)\n",
    "        # self.emb2 = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=size,\n",
    "        #                       stride=1, dilation=1, padding=pad, bias=False)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "        init_bn(self.bn1)\n",
    "        init_bn(self.bn2)\n",
    "        init_emb(self.emb1)\n",
    "        init_emb(self.emb2)\n",
    "\n",
    "    # latent query embedded \n",
    "    def forward(self, x, condition):\n",
    "        c1 = self.emb1(condition)\n",
    "        c2 = self.emb2(condition)\n",
    "        x = act(self.bn1(self.conv1(x)), self.activation) + c1[:, :, None, None]\n",
    "        x = act(self.bn2(self.conv2(x)), self.activation) + c2[:, :, None, None]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample, activation, momentum, classes_num = 527):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        size = 3\n",
    "\n",
    "        self.conv_block = ConvBlock(in_channels, out_channels, size, activation, momentum, classes_num)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        encoder = self.conv_block(x, condition)\n",
    "        encoder_pool = F.avg_pool2d(encoder, kernel_size=self.downsample)\n",
    "        return encoder_pool, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "y, _ = librosa.load(\"/Users/cooky/HDD/Drum/tactspack/drum loops/drumroll 12_sel.wav\", mono=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = y.reshape(1,y.shape[0],1)\n",
    "input = np.array([y,y,y,y,y]).reshape(5,y.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 18462, 1)"
      ]
     },
     "execution_count": 804,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor(input, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 18462, 1])"
      ]
     },
     "execution_count": 806,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = wav_to_spectrogram(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sp.transpose(1,3)\n",
    "x = bn0(x)\n",
    "x = x.transpose(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 58, 1025])"
      ]
     },
     "execution_count": 809,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_len = x.shape[2]\n",
    "pad_len = int(np.ceil(x.shape[2] / downsample_ratio)) \\\n",
    "    * downsample_ratio - origin_len\n",
    "x = F.pad(x, pad=(0, 0, 0, pad_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 64, 1025])"
      ]
     },
     "execution_count": 811,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[..., 0 : x.shape[-1] - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 64, 1024])"
      ]
     },
     "execution_count": 813,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_block1 = EncoderBlock(in_channels=channels, out_channels=32, \n",
    "            downsample=(2, 2), activation=activation, momentum=momentum, classes_num = 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = torch.tensor(np.zeros((1,99)),dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 5, 3, 3], expected input[1, 1, 64, 1024] to have 5 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [720]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mencoder_block1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [441]\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[0;34m(self, x, condition)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, condition):\n\u001b[0;32m---> 10\u001b[0m     encoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     encoder_pool \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mavg_pool2d(encoder, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoder_pool, encoder\n",
      "File \u001b[0;32m~/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [440]\u001b[0m, in \u001b[0;36mConvBlock.forward\u001b[0;34m(self, x, condition)\u001b[0m\n\u001b[1;32m     42\u001b[0m c1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb1(condition)\n\u001b[1;32m     43\u001b[0m c2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb2(condition)\n\u001b[0;32m---> 44\u001b[0m x \u001b[38;5;241m=\u001b[39m act(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation) \u001b[38;5;241m+\u001b[39m c1[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m     45\u001b[0m x \u001b[38;5;241m=\u001b[39m act(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation) \u001b[38;5;241m+\u001b[39m c2[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 5, 3, 3], expected input[1, 1, 64, 1024] to have 5 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "encoder_block1(x, condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "size = 3\n",
    "pad = size//2\n",
    "conv_block = ConvBlock(channels, 32, size, activation, momentum, 99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(in_channels=1, \n",
    "                              out_channels=5,\n",
    "                              kernel_size=(size, size), stride=(1, 1), \n",
    "                              dilation=(1, 1), padding=(pad, pad), bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5, 64, 1024])"
      ]
     },
     "execution_count": 830,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn1 = nn.BatchNorm2d(5, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = act(bn1(conv1(x)), activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5, 64, 1024])"
      ]
     },
     "execution_count": 835,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1 = nn.Linear(99, 5, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_imsi = nn.Conv1d(in_channels=channels, \n",
    "                              out_channels=5*5,\n",
    "                              kernel_size=size, stride=1,\n",
    "                              dilation=1, padding=pad, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = torch.tensor(np.zeros((5,99)),dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = emb1(condition)\n",
    "# c1 = emb_imsi(condition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 840,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5, 1, 1])"
      ]
     },
     "execution_count": 842,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c11 = c1[:,:,None,None]\n",
    "c11.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5, 64, 1024])"
      ]
     },
     "execution_count": 843,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x1 + c11).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, size, activation, momentum, classes_num = 527):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        pad = size // 2\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(size, size), stride=(1, 1), \n",
    "                              dilation=(1, 1), padding=(pad, pad), bias=False)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels, momentum=momentum)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(size, size), stride=(1, 1), \n",
    "                              dilation=(1, 1), padding=(pad, pad), bias=False)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels, momentum=momentum)\n",
    "        # change autotagging size\n",
    "        #####\n",
    "        self.emb1 = nn.Linear(classes_num, out_channels, bias=True)\n",
    "        self.emb2 = nn.Linear(classes_num, out_channels, bias=True)\n",
    "        ####\n",
    "        # self.emb1 = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=size,\n",
    "        #                       stride=1, dilation=1, padding=pad, bias=False)\n",
    "        # self.emb2 = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=size,\n",
    "        #                       stride=1, dilation=1, padding=pad, bias=False)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "        init_bn(self.bn1)\n",
    "        init_bn(self.bn2)\n",
    "        init_emb(self.emb1)\n",
    "        init_emb(self.emb2)\n",
    "\n",
    "    # latent query embedded \n",
    "    def forward(self, x, condition):\n",
    "        c1 = self.emb1(condition)\n",
    "        c2 = self.emb2(condition)\n",
    "        x = act(self.bn1(self.conv1(x)), self.activation) + c1[:, :, None, None]\n",
    "        x = act(self.bn2(self.conv2(x)), self.activation) + c2[:, :, None, None]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1024) must match the size of tensor b (32) at non-singleton dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [263]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mconv_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [192]\u001b[0m, in \u001b[0;36mConvBlock.forward\u001b[0;34m(self, x, condition)\u001b[0m\n\u001b[1;32m     42\u001b[0m c1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb1(condition)\n\u001b[1;32m     43\u001b[0m c2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb2(condition)\n\u001b[0;32m---> 44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc1\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\n\u001b[1;32m     45\u001b[0m x \u001b[38;5;241m=\u001b[39m act(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation) \u001b[38;5;241m+\u001b[39m c2[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1024) must match the size of tensor b (32) at non-singleton dimension 4"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder = conv_block(x, condition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [264]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoder_pool \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mavg_pool2d(\u001b[43mencoder\u001b[49m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoder_pool, encoder\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "encoder_pool = F.avg_pool2d(encoder, kernel_size=self.downsample)\n",
    "return encoder_pool, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvggish.vggish as vggish\n",
    "\n",
    "model_urls = {\n",
    "    'vggish': 'https://github.com/harritaylor/torchvggish/'\n",
    "              'releases/download/v0.1/vggish-10086976.pth',\n",
    "    'pca': 'https://github.com/harritaylor/torchvggish/'\n",
    "           'releases/download/v0.1/vggish_pca_params-970ea276.pth'\n",
    "}\n",
    "model_urls = {\n",
    "    'vggish': './torchvggish/cp/vggish-10086976.pth',\n",
    "    'pca': './torchvggish/cp/vggish_pca_params-970ea276.pth'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vggish.VGGish(model_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/Users/cooky/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torchaudio/lib/libtorchaudio.so, 0x0006): Symbol not found: __ZN2at14RecordFunctionC1ENS_11RecordScopeEb\n  Referenced from: <E741B6D5-E348-3601-ACC9-BC3101AD112C> /Users/cooky/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torchaudio/lib/libtorchaudio.so\n  Expected in:     <AAE88793-2D9D-3CCA-96C4-EAC30CEA4202> /Users/cooky/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprototype\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipelines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mVGGishBundle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mVGGish\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mVGGish\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torchaudio/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m      2\u001b[0m     _extension,\n\u001b[1;32m      3\u001b[0m     compliance,\n\u001b[1;32m      4\u001b[0m     datasets,\n\u001b[1;32m      5\u001b[0m     functional,\n\u001b[1;32m      6\u001b[0m     io,\n\u001b[1;32m      7\u001b[0m     kaldi_io,\n\u001b[1;32m      8\u001b[0m     models,\n\u001b[1;32m      9\u001b[0m     pipelines,\n\u001b[1;32m     10\u001b[0m     sox_effects,\n\u001b[1;32m     11\u001b[0m     transforms,\n\u001b[1;32m     12\u001b[0m     utils,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_audio_backend, list_audio_backends, set_audio_backend\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torchaudio/_extension.py:101\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m \u001b[43m_init_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torchaudio/_extension.py:86\u001b[0m, in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchaudio C++ extension is not available.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtorchaudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchaudio  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torchaudio/_extension.py:51\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m torch\u001b[38;5;241m.\u001b[39mclasses\u001b[38;5;241m.\u001b[39mload_library(path)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torch/_ops.py:260\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    255\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[0;32m~/miniforge3/envs/cid2rch/lib/python3.10/ctypes/__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/Users/cooky/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torchaudio/lib/libtorchaudio.so, 0x0006): Symbol not found: __ZN2at14RecordFunctionC1ENS_11RecordScopeEb\n  Referenced from: <E741B6D5-E348-3601-ACC9-BC3101AD112C> /Users/cooky/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torchaudio/lib/libtorchaudio.so\n  Expected in:     <AAE88793-2D9D-3CCA-96C4-EAC30CEA4202> /Users/cooky/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib"
     ]
    }
   ],
   "source": [
    "import torchaudio.prototype.pipelines.VGGishBundle.VGGish as VGGish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooky/miniforge3/envs/cid2rch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "dlopen(/Users/cooky/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torchaudio/lib/libtorchaudio.so, 0x0006): Symbol not found: __ZN2at14RecordFunctionC1ENS_11RecordScopeEb\n  Referenced from: <E741B6D5-E348-3601-ACC9-BC3101AD112C> /Users/cooky/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torchaudio/lib/libtorchaudio.so\n  Expected in:     <AAE88793-2D9D-3CCA-96C4-EAC30CEA4202> /Users/cooky/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# from losses import get_loss_func\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torchaudio/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m      2\u001b[0m     _extension,\n\u001b[1;32m      3\u001b[0m     compliance,\n\u001b[1;32m      4\u001b[0m     datasets,\n\u001b[1;32m      5\u001b[0m     functional,\n\u001b[1;32m      6\u001b[0m     io,\n\u001b[1;32m      7\u001b[0m     kaldi_io,\n\u001b[1;32m      8\u001b[0m     models,\n\u001b[1;32m      9\u001b[0m     pipelines,\n\u001b[1;32m     10\u001b[0m     sox_effects,\n\u001b[1;32m     11\u001b[0m     transforms,\n\u001b[1;32m     12\u001b[0m     utils,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_audio_backend, list_audio_backends, set_audio_backend\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torchaudio/_extension.py:101\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m \u001b[43m_init_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torchaudio/_extension.py:86\u001b[0m, in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchaudio C++ extension is not available.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtorchaudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchaudio  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torchaudio/_extension.py:51\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m torch\u001b[38;5;241m.\u001b[39mclasses\u001b[38;5;241m.\u001b[39mload_library(path)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torch/_ops.py:260\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    255\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[0;32m~/miniforge3/envs/cid2rch/lib/python3.10/ctypes/__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/Users/cooky/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torchaudio/lib/libtorchaudio.so, 0x0006): Symbol not found: __ZN2at14RecordFunctionC1ENS_11RecordScopeEb\n  Referenced from: <E741B6D5-E348-3601-ACC9-BC3101AD112C> /Users/cooky/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torchaudio/lib/libtorchaudio.so\n  Expected in:     <AAE88793-2D9D-3CCA-96C4-EAC30CEA4202> /Users/cooky/miniforge3/envs/cid2rch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib"
     ]
    }
   ],
   "source": [
    "# Localize All Around Sounds\n",
    "\n",
    "\n",
    "from museval.metrics import validate\n",
    "from numba.core.types.containers import DictKeysIterableType\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import bisect\n",
    "import pickle\n",
    "import soundfile as sf\n",
    "import subprocess\n",
    "\n",
    "import noisereduce as nr\n",
    "from asp.utils import get_segment_bgn_end_samples, np_to_pytorch, get_mix_data, evaluate_sdr, wiener, split_nparray_with_overlap\n",
    "# from losses import get_loss_func\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as cp\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.distributed as dist\n",
    "from torchlibrosa.stft import STFT, ISTFT, magphase\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "#####\n",
    "import torchvggish.vggish as vggish\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def init_layer(layer):\n",
    "    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    " \n",
    "    if hasattr(layer, 'bias'):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_emb(layer):\n",
    "    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n",
    "    nn.init.uniform_(layer.weight, -0.1, 0.1)\n",
    " \n",
    "    if hasattr(layer, 'bias'):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    \"\"\"Initialize a Batchnorm layer. \"\"\"\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.)\n",
    "\n",
    "\n",
    "def init_gru(rnn):\n",
    "    \"\"\"Initialize a GRU layer. \"\"\"\n",
    "    \n",
    "    def _concat_init(tensor, init_funcs):\n",
    "        (length, fan_out) = tensor.shape\n",
    "        fan_in = length // len(init_funcs)\n",
    "    \n",
    "        for (i, init_func) in enumerate(init_funcs):\n",
    "            init_func(tensor[i * fan_in : (i + 1) * fan_in, :])\n",
    "        \n",
    "    def _inner_uniform(tensor):\n",
    "        fan_in = nn.init._calculate_correct_fan(tensor, 'fan_in')\n",
    "        nn.init.uniform_(tensor, -math.sqrt(3 / fan_in), math.sqrt(3 / fan_in))\n",
    "    \n",
    "    for i in range(rnn.num_layers):\n",
    "        _concat_init(\n",
    "            getattr(rnn, 'weight_ih_l{}'.format(i)),\n",
    "            [_inner_uniform, _inner_uniform, _inner_uniform]\n",
    "        )\n",
    "        torch.nn.init.constant_(getattr(rnn, 'bias_ih_l{}'.format(i)), 0)\n",
    "\n",
    "        _concat_init(\n",
    "            getattr(rnn, 'weight_hh_l{}'.format(i)),\n",
    "            [_inner_uniform, _inner_uniform, nn.init.orthogonal_]\n",
    "        )\n",
    "        torch.nn.init.constant_(getattr(rnn, 'bias_hh_l{}'.format(i)), 0)\n",
    "\n",
    "\n",
    "def act(x, activation):\n",
    "    if activation == 'relu':\n",
    "        return F.relu_(x)\n",
    "\n",
    "    elif activation == 'leaky_relu':\n",
    "        return F.leaky_relu_(x, negative_slope=0.2)\n",
    "\n",
    "    elif activation == 'swish':\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "    else:\n",
    "        raise Exception('Incorrect activation!')\n",
    "\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, size, activation, momentum, classes_num = 128):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        pad = size // 2\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(size, size), stride=(1, 1), \n",
    "                              dilation=(1, 1), padding=(pad, pad), bias=False)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels, momentum=momentum)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(size, size), stride=(1, 1), \n",
    "                              dilation=(1, 1), padding=(pad, pad), bias=False)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels, momentum=momentum)\n",
    "        # change autotagging size\n",
    "        self.emb1 = nn.Linear(classes_num, out_channels, bias=True)\n",
    "        self.emb2 = nn.Linear(classes_num, out_channels, bias=True)\n",
    "        #####\n",
    "        # self.emb_conv1 = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=size,\n",
    "        #                       stride=1, dilation=1, padding=pad, bias=False)\n",
    "        # self.emb_conv2 = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=size,\n",
    "        #                       stride=1, dilation=1, padding=pad, bias=False)\n",
    "        #####\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "        init_bn(self.bn1)\n",
    "        init_bn(self.bn2)\n",
    "        init_emb(self.emb1)\n",
    "        init_emb(self.emb2)\n",
    "\n",
    "    # latent query embedded \n",
    "    def forward(self, x, condition):\n",
    "        # c1 = self.emb1(condition)\n",
    "        # c2 = self.emb2(condition)\n",
    "        # x = act(self.bn1(self.conv1(x)), self.activation) + c1[:, :, None, None]\n",
    "        # x = act(self.bn2(self.conv2(x)), self.activation) + c2[:, :, None, None]\n",
    "        #####\n",
    "        c1_ = self.emb_conv1(condition)\n",
    "        c1 = self.emb1(c1_)\n",
    "        c2_ = self.emb_conv2(condition)\n",
    "        c2 = self.emb2(c2_)\n",
    "        x = act(self.bn1(self.conv1(x)), self.activation) + c1[:, :, None]\n",
    "        x = act(self.bn2(self.conv2(x)), self.activation) + c2[:, :, None]\n",
    "        #####\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample, activation, momentum, classes_num = 527):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        size = 3\n",
    "\n",
    "        self.conv_block = ConvBlock(in_channels, out_channels, size, activation, momentum, classes_num)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        encoder = self.conv_block(x, condition)\n",
    "        encoder_pool = F.avg_pool2d(encoder, kernel_size=self.downsample)\n",
    "        return encoder_pool, encoder\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, activation, momentum, classes_num = 527):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        size = 3\n",
    "        self.activation = activation\n",
    "\n",
    "        self.conv1 = torch.nn.ConvTranspose2d(in_channels=in_channels, \n",
    "            out_channels=out_channels, kernel_size=(size, size), stride=stride, \n",
    "            padding=(0, 0), output_padding=(0, 0), bias=False, dilation=(1, 1))\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels, momentum=momentum)\n",
    "        self.conv_block2 = ConvBlock(out_channels * 2, out_channels, size, activation, momentum, classes_num)\n",
    "        # change autotagging size\n",
    "        self.emb1 = nn.Linear(classes_num, out_channels, bias=True)\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_bn(self.bn)\n",
    "        init_emb(self.emb1)\n",
    "\n",
    "    def prune(self, x):\n",
    "        \"\"\"Prune the shape of x after transpose convolution.\n",
    "        \"\"\"\n",
    "        x = x[:, :, 0 : - 1, 0 : - 1]\n",
    "        return x\n",
    "\n",
    "    def forward(self, input_tensor, concat_tensor, condition):\n",
    "        c1 = self.emb1(condition)\n",
    "        x = act(self.bn1(self.conv1(input_tensor)), self.activation) + c1[:, :, None, None]\n",
    "        x = self.prune(x)\n",
    "        x = torch.cat((x, concat_tensor), dim=1)\n",
    "        x = self.conv_block2(x, condition)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class ZeroShotASP(pl.LightningModule):\n",
    "    '''\n",
    "    Args:\n",
    "    channels (int): the audio channel, default:1 (mono)\n",
    "    config (module): the configuration module as in config.py\n",
    "    at_model (module): the sound event detection system\n",
    "    dataset (module): the dataset variable to control the randomness in each epoch (not affect in evaluation mode) \n",
    "    '''\n",
    "    def __init__(self, config, at_model, dataset, channels=100):\n",
    "        super().__init__()\n",
    "\n",
    "        # hyper parameters\n",
    "        window_size = 2048\n",
    "        hop_size = config.hop_samples\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        window = 'hann'\n",
    "        activation = 'relu'\n",
    "        momentum = 0.01\n",
    "        self.check_flag = False\n",
    "        #####\n",
    "        self.channels=channels\n",
    "        #####\n",
    "        self.config = config\n",
    "        self.at_model = at_model\n",
    "        self.opt_thres = pickle.load(open('opt_thres.pkl', 'rb'))\n",
    "        self.loss_func = get_loss_func(self.config.loss_type)\n",
    "        self.dataset = dataset\n",
    "        if self.config.using_whiting:\n",
    "            temp = np.load(\"whiting_weight.npy\", allow_pickle=True)\n",
    "            temp = temp.item()\n",
    "            self.whiting_kernel = temp[\"kernel\"]\n",
    "            self.whiting_bias = temp[\"bias\"]\n",
    "\n",
    "        self.downsample_ratio = 2 ** 6   # This number equals 2^{#encoder_blcoks}\n",
    "\n",
    "        self.stft = STFT(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, \n",
    "            pad_mode=pad_mode, freeze_parameters=True)\n",
    "\n",
    "        self.istft = ISTFT(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, \n",
    "            pad_mode=pad_mode, freeze_parameters=True)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(window_size // 2 + 1, momentum=momentum)\n",
    "\n",
    "        self.encoder_block1 = EncoderBlock(in_channels=channels, out_channels=32*channels, \n",
    "            downsample=(2, 2), activation=activation, momentum=momentum, classes_num = config.latent_dim)\n",
    "        self.encoder_block2 = EncoderBlock(in_channels=32, out_channels=64, \n",
    "            downsample=(2, 2), activation=activation, momentum=momentum, classes_num = config.latent_dim)\n",
    "        self.encoder_block3 = EncoderBlock(in_channels=64, out_channels=128, \n",
    "            downsample=(2, 2), activation=activation, momentum=momentum, classes_num = config.latent_dim)\n",
    "        self.encoder_block4 = EncoderBlock(in_channels=128, out_channels=256, \n",
    "            downsample=(2, 2), activation=activation, momentum=momentum, classes_num = config.latent_dim)\n",
    "        self.encoder_block5 = EncoderBlock(in_channels=256, out_channels=512, \n",
    "            downsample=(2, 2), activation=activation, momentum=momentum, classes_num = config.latent_dim)\n",
    "        self.encoder_block6 = EncoderBlock(in_channels=512, out_channels=1024, \n",
    "            downsample=(2, 2), activation=activation, momentum=momentum, classes_num = config.latent_dim)\n",
    "        self.conv_block7 = ConvBlock(in_channels=1024, out_channels=2048, \n",
    "            size=3, activation=activation, momentum=momentum, classes_num = config.latent_dim)\n",
    "        self.decoder_block1 = DecoderBlock(in_channels=2048, out_channels=1024, \n",
    "            stride=(2, 2), activation=activation, momentum=momentum, classes_num = config.latent_dim)\n",
    "        self.decoder_block2 = DecoderBlock(in_channels=1024, out_channels=512, \n",
    "            stride=(2, 2), activation=activation, momentum=momentum, classes_num = config.latent_dim)\n",
    "        self.decoder_block3 = DecoderBlock(in_channels=512, out_channels=256, \n",
    "            stride=(2, 2), activation=activation, momentum=momentum, classes_num = config.latent_dim)\n",
    "        self.decoder_block4 = DecoderBlock(in_channels=256, out_channels=128, \n",
    "            stride=(2, 2), activation=activation, momentum=momentum, classes_num = config.latent_dim)\n",
    "        self.decoder_block5 = DecoderBlock(in_channels=128, out_channels=64, \n",
    "            stride=(2, 2), activation=activation, momentum=momentum, classes_num = config.latent_dim)\n",
    "        self.decoder_block6 = DecoderBlock(in_channels=64, out_channels=32, \n",
    "            stride=(2, 2), activation=activation, momentum=momentum, classes_num = config.latent_dim)\n",
    "\n",
    "        self.after_conv_block1 = ConvBlock(in_channels=32, out_channels=32, \n",
    "            size=3, activation=activation, momentum=momentum, classes_num = config.latent_dim)\n",
    "\n",
    "        self.after_conv2 = nn.Conv2d(in_channels=32, out_channels=channels, \n",
    "            kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.after_conv2)\n",
    "\n",
    "    def spectrogram(self, input):\n",
    "        (real, imag) = self.stft(input)\n",
    "        return (real ** 2 + imag ** 2) ** 0.5\n",
    "\n",
    "    def wav_to_spectrogram(self, input):\n",
    "        \"\"\"Waveform to spectrogram.\n",
    "\n",
    "        Args:\n",
    "          input: (batch_size, segment_samples, channels_num)\n",
    "\n",
    "        Outputs:\n",
    "          output: (batch_size, channels_num, time_steps, freq_bins)\n",
    "        \"\"\"\n",
    "        sp_list = []\n",
    "        #####\n",
    "        # channels_num = input.shape[2]\n",
    "        # for channel in range(channels_num):\n",
    "        #     sp_list.append(self.spectrogram(input[:, :, channel]))\n",
    "        #####\n",
    "        for _ in range(self.channels):\n",
    "            sp_list.append(self.spectrogram(input[:,:,0]))\n",
    "\n",
    "        output = torch.cat(sp_list, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def spectrogram_to_wav(self, input, spectrogram, length=None):\n",
    "        \"\"\"Spectrogram to waveform.\n",
    "\n",
    "        Args:\n",
    "          input: (batch_size, segment_samples, channels_num)\n",
    "          spectrogram: (batch_size, channels_num, time_steps, freq_bins)\n",
    "\n",
    "        Outputs:\n",
    "          output: (batch_size, segment_samples, channels_num)\n",
    "        \"\"\"\n",
    "        channels_num = input.shape[2]\n",
    "        wav_list = []\n",
    "        for channel in range(channels_num):\n",
    "            (real, imag) = self.stft(input[:, :, channel])\n",
    "            (_, cos, sin) = magphase(real, imag)\n",
    "            wav_list.append(self.istft(spectrogram[:, channel : channel + 1, :, :] * cos, \n",
    "                spectrogram[:, channel : channel + 1, :, :] * sin, length))\n",
    "        \n",
    "        output = torch.stack(wav_list, dim=2)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input, condition):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input: (batch_size, segment_samples, channels_num)\n",
    "\n",
    "        Outputs:\n",
    "          output_dict: {\n",
    "            'wav': (batch_size, segment_samples, channels_num),\n",
    "            'sp': (batch_size, channels_num, time_steps, freq_bins)}\n",
    "        \"\"\"\n",
    "        sp = self.wav_to_spectrogram(input)    \n",
    "        \"\"\"(batch_size, channels_num, time_steps, freq_bins)\"\"\"\n",
    "\n",
    "        # Batch normalization\n",
    "        x = sp.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \"\"\"(batch_size, chanenls, time_steps, freq_bins)\"\"\"\n",
    "\n",
    "        # Pad spectrogram to be evenly divided by downsample ratio.\n",
    "        origin_len = x.shape[2]\n",
    "        pad_len = int(np.ceil(x.shape[2] / self.downsample_ratio)) \\\n",
    "            * self.downsample_ratio - origin_len\n",
    "        x = F.pad(x, pad=(0, 0, 0, pad_len))\n",
    "        \"\"\"(batch_size, channels, padded_time_steps, freq_bins)\"\"\"\n",
    "\n",
    "        # Let frequency bins be evenly divided by 2, e.g., 513 -> 512\n",
    "        x = x[..., 0 : x.shape[-1] - 1]     # (bs, channels, T, F)\n",
    "\n",
    "        # UNet\n",
    "        (x1_pool, x1) = self.encoder_block1(x, condition)  # x1_pool: (bs, 32, T / 2, F / 2)\n",
    "        (x2_pool, x2) = self.encoder_block2(x1_pool, condition)    # x2_pool: (bs, 64, T / 4, F / 4)\n",
    "        (x3_pool, x3) = self.encoder_block3(x2_pool, condition)    # x3_pool: (bs, 128, T / 8, F / 8)\n",
    "        (x4_pool, x4) = self.encoder_block4(x3_pool, condition)    # x4_pool: (bs, 256, T / 16, F / 16)\n",
    "        (x5_pool, x5) = self.encoder_block5(x4_pool, condition)    # x5_pool: (bs, 512, T / 32, F / 32)\n",
    "        (x6_pool, x6) = self.encoder_block6(x5_pool, condition)    # x6_pool: (bs, 1024, T / 64, F / 64)\n",
    "        x_center = self.conv_block7(x6_pool, condition)    # (bs, 2048, T / 64, F / 64)\n",
    "        x7 = self.decoder_block1(x_center, x6, condition)  # (bs, 1024, T / 32, F / 32)\n",
    "        x8 = self.decoder_block2(x7, x5, condition)    # (bs, 512, T / 16, F / 16)\n",
    "        x9 = self.decoder_block3(x8, x4, condition)    # (bs, 256, T / 8, F / 8)\n",
    "        x10 = self.decoder_block4(x9, x3, condition)   # (bs, 128, T / 4, F / 4)\n",
    "        x11 = self.decoder_block5(x10, x2, condition)  # (bs, 64, T / 2, F / 2)\n",
    "        x12 = self.decoder_block6(x11, x1, condition)  # (bs, 32, T, F)\n",
    "        x = self.after_conv_block1(x12, condition)     # (bs, 32, T, F)\n",
    "        x = self.after_conv2(x)             # (bs, channels, T, F)\n",
    "\n",
    "        # Recover shape\n",
    "        x = F.pad(x, pad=(0, 1))\n",
    "        x = x[:, :, 0 : origin_len, :]\n",
    "\n",
    "        sp_out = torch.sigmoid(x) * sp\n",
    "\n",
    "        # Spectrogram to wav\n",
    "        length = input.shape[1]\n",
    "        wav_out = self.spectrogram_to_wav(input, sp_out, length)\n",
    "\n",
    "        output_dict = {\"wav\": wav_out, \"sp\": sp_out}\n",
    "        return output_dict\n",
    "\n",
    "    def get_new_indexes(self, x):\n",
    "        indexes = [*range(x.shape[0])]\n",
    "        return indexes\n",
    "    def get_auto_tagging(self, data):\n",
    "        waveforms = data[\"waveform\"]\n",
    "        class_ids = data[\"class_id\"]\n",
    "        audio_num = len(waveforms)\n",
    "        at_waveforms = []\n",
    "        output_dicts = self.at_model.inference(waveforms) # B, T, C\n",
    "        sed_vectors = output_dicts[\"framewise_output\"]\n",
    "        for i in range(audio_num):\n",
    "            # obtain the sed_vector\n",
    "            sed_vector = np.convolve(\n",
    "                sed_vectors[i, :, class_ids[i]], np.ones(self.config.segment_frames),\n",
    "                mode = \"same\"\n",
    "            )\n",
    "            anchor_index = math.floor(np.argmax(sed_vector) * self.config.clip_samples / self.config.hop_samples / 1024)\n",
    "            (bgn_sample, end_sample) = get_segment_bgn_end_samples(\n",
    "                anchor_index, self.config.segment_frames,\n",
    "                self.config.hop_samples, self.config.clip_samples\n",
    "            )\n",
    "            at_waveforms.append(waveforms[i, bgn_sample: end_sample])\n",
    "        at_waveforms = np.array(at_waveforms)\n",
    "        output_dicts = self.at_model.inference(at_waveforms)\n",
    "        at_vectors = output_dicts[\"latent_output\"]\n",
    "        return at_waveforms, at_vectors\n",
    "\n",
    "\n",
    "    def combine_batch(self, x, y):\n",
    "        xy = []\n",
    "        assert len(x) == len(y), \"two combined batches should be in the same length\"\n",
    "        for i in range(len(x)):\n",
    "            xy += [x[i], y[i]]\n",
    "        return np.array(xy)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.train()\n",
    "        self.device_type = next(self.parameters()).device\n",
    "        if not self.check_flag:\n",
    "            self.check_flag = True\n",
    "\n",
    "        combine_batch = {}\n",
    "        combine_batch[\"class_id\"] = self.combine_batch(batch[\"class_id_1\"], batch[\"class_id_2\"])\n",
    "        combine_batch[\"waveform\"] = self.combine_batch(batch[\"waveform_1\"], batch[\"waveform_2\"])\n",
    "        \n",
    "        # laten embedding from the sound event detection/auto tagging system\n",
    "        at_waveforms, at_vectors = self.get_auto_tagging(combine_batch)\n",
    "        tmp = np.zeros_like(at_vectors) # [batch, classes_num]\n",
    "\n",
    "        indexes = self.get_new_indexes(tmp)\n",
    "        if self.config.using_whiting:\n",
    "            at_vectors = (at_vectors + self.whiting_bias).dot(self.whiting_kernel)\n",
    "            at_vectors = at_vectors[:,:self.config.latent_dim]\n",
    "\n",
    "        # define input data by mixing\n",
    "        mixtures, sources, conditions, _ = get_mix_data(\n",
    "            at_waveforms, at_vectors, combine_batch[\"class_id\"], indexes,\n",
    "            mix_type = \"mixture\"\n",
    "        )\n",
    "        if len(mixtures) > 0:\n",
    "            # conver to tensor\n",
    "            mixtures = np_to_pytorch(np.array(mixtures)[:, :, None], self.device_type)\n",
    "            sources = np_to_pytorch(np.array(sources)[:, :, None], self.device_type)\n",
    "            conditions = np_to_pytorch(np.array(conditions), self.device_type)\n",
    "            # train\n",
    "            batch_output_dict = self(mixtures, conditions)\n",
    "            loss = self.loss_func(batch_output_dict[\"wav\"], sources)\n",
    "            return loss\n",
    "        else:\n",
    "            return None\n",
    "    def training_epoch_end(self, outputs):\n",
    "        self.dataset.generate_queue()\n",
    "        self.check_flag = False\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        mixture_sdr = []\n",
    "        clean_sdr = []\n",
    "        silence_sdr = []\n",
    "        self.device_type = next(self.parameters()).device\n",
    "        combine_batch = {}\n",
    "        combine_batch[\"class_id\"] = self.combine_batch(batch[\"class_id_1\"], batch[\"class_id_2\"])\n",
    "        combine_batch[\"waveform\"] = self.combine_batch(batch[\"waveform_1\"], batch[\"waveform_2\"])\n",
    "        \n",
    "        # laten embedding from the sound event detection/auto tagging system\n",
    "        at_waveforms, at_vectors = self.get_auto_tagging(combine_batch)\n",
    "        tmp = np.zeros_like(at_vectors) # [batch, classes_num]\n",
    "        # new un-conflict indexes \n",
    "        indexes = self.get_new_indexes(tmp)\n",
    "        if self.config.using_whiting:\n",
    "            at_vectors = (at_vectors + self.whiting_bias).dot(self.whiting_kernel)\n",
    "            at_vectors = at_vectors[:,:self.config.latent_dim]\n",
    "\n",
    "        # define mixture data\n",
    "        mixtures, sources, conditions, gds = get_mix_data(\n",
    "            at_waveforms, at_vectors, combine_batch[\"class_id\"], indexes,\n",
    "            mix_type = \"mixture\"\n",
    "        )\n",
    "        if len(mixtures) > 0:\n",
    "            # conver to tensor\n",
    "            mixtures = np_to_pytorch(np.array(mixtures)[:, :, None], self.device_type)\n",
    "            sources = np_to_pytorch(np.array(sources)[:, :, None], self.device_type)\n",
    "            conditions = np_to_pytorch(np.array(conditions), self.device_type)\n",
    "            gds = np_to_pytorch(np.array(gds), self.device_type)\n",
    "            # train\n",
    "            batch_output_dict = self(mixtures, conditions)\n",
    "            preds = batch_output_dict[\"wav\"]\n",
    "        \n",
    "            mixture_sdr = evaluate_sdr(\n",
    "                ref = sources.data.cpu().numpy(), \n",
    "                est = preds.data.cpu().numpy(),\n",
    "                class_ids = gds.data.cpu().numpy(),\n",
    "                mix_type = \"mixture\"\n",
    "            )\n",
    "            \n",
    "        # define clean data\n",
    "        mixtures, sources, conditions, gds = get_mix_data(\n",
    "            at_waveforms, at_vectors, combine_batch[\"class_id\"], indexes,\n",
    "            mix_type = \"clean\"\n",
    "        )\n",
    "        if len(mixtures) > 0:\n",
    "            # conver to tensor\n",
    "            mixtures = np_to_pytorch(np.array(mixtures)[:, :, None], self.device_type)\n",
    "            sources = np_to_pytorch(np.array(sources)[:, :, None], self.device_type)\n",
    "            conditions = np_to_pytorch(np.array(conditions), self.device_type)\n",
    "            gds = np_to_pytorch(np.array(gds), self.device_type)\n",
    "            # train\n",
    "            batch_output_dict = self(mixtures, conditions)\n",
    "            preds = batch_output_dict[\"wav\"]\n",
    "        \n",
    "            clean_sdr = evaluate_sdr(\n",
    "                ref = sources.data.cpu().numpy(), \n",
    "                est = preds.data.cpu().numpy(),\n",
    "                class_ids = gds.data.cpu().numpy(),\n",
    "                mix_type = \"clean\"\n",
    "            )   \n",
    "        # define mixture data\n",
    "        mixtures, sources, conditions, gds = get_mix_data(\n",
    "            at_waveforms, at_vectors, combine_batch[\"class_id\"], indexes,\n",
    "            mix_type = \"silence\"\n",
    "        )\n",
    "        if len(mixtures) > 0:\n",
    "            # conver to tensor\n",
    "            mixtures = np_to_pytorch(np.array(mixtures)[:, :, None], self.device_type)\n",
    "            sources = np_to_pytorch(np.array(sources)[:, :, None], self.device_type)\n",
    "            conditions = np_to_pytorch(np.array(conditions), self.device_type)\n",
    "            gds = np_to_pytorch(np.array(gds), self.device_type)\n",
    "            # train\n",
    "            batch_output_dict = self(mixtures, conditions)\n",
    "            preds = batch_output_dict[\"wav\"]\n",
    "            silence_sdr = evaluate_sdr(\n",
    "                ref = mixtures.data.cpu().numpy(), \n",
    "                est = preds.data.cpu().numpy(),\n",
    "                class_ids = gds.data.cpu().numpy(),\n",
    "                mix_type = \"silence\"\n",
    "            )\n",
    "        return {\"mixture\": mixture_sdr, \"clean\": clean_sdr, \"silence\": silence_sdr}\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        self.device_type = next(self.parameters()).device\n",
    "        mixture_sdr = []\n",
    "        clean_sdr = []\n",
    "        silence_sdr = []\n",
    "        for d in validation_step_outputs:\n",
    "            mixture_sdr += [dd[0] for dd in d[\"mixture\"]]\n",
    "            clean_sdr += [dd[0] for dd in d[\"clean\"]]\n",
    "            silence_sdr += [dd[0] for dd in d[\"silence\"]]\n",
    "        mixture_sdr = np.mean(np.array(mixture_sdr))\n",
    "        clean_sdr = np.mean(np.array(clean_sdr))\n",
    "        silence_sdr = np.mean(np.array(silence_sdr))\n",
    "        \n",
    "        self.log(\"mixture_sdr\", mixture_sdr, on_epoch = True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"clean_sdr\", clean_sdr, on_epoch = True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"silence_sdr\", silence_sdr, on_epoch = True, prog_bar=True, sync_dist=True)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        self.validation_epoch_end(test_step_outputs)             \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(\n",
    "            self.parameters(), lr = self.config.learning_rate, \n",
    "            betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0., amsgrad = True\n",
    "        )\n",
    "        def lr_foo(epoch):       \n",
    "            if epoch < 3:\n",
    "                # warm up lr\n",
    "                lr_scale = 0.1 ** (3 - epoch)\n",
    "            else:\n",
    "                lr_scale = 0.1 ** (bisect.bisect_left(self.config.lr_scheduler_epoch, epoch))\n",
    "\n",
    "            return lr_scale\n",
    "        scheduler = optim.lr_scheduler.LambdaLR(\n",
    "            optimizer,\n",
    "            lr_lambda=lr_foo\n",
    "        )\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
