{"cells":[{"cell_type":"markdown","metadata":{"id":"p35qYqUBUAoj"},"source":["# Environment setup"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["# ! python -m pip install detectron2 -f \\\n","#   https://dl.fbaipublicfiles.com/detectron2/wheels/cu113/torch1.10/index.html"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44,"status":"ok","timestamp":1706448529747,"user":{"displayName":"구은아","userId":"03420424854508517452"},"user_tz":-540},"id":"qvh1F6NuUAow","outputId":"3882b3b6-45b6-4493-c653-0093519c6b0b"},"outputs":[{"name":"stdout","output_type":"stream","text":["/workspace/GitHub/AVSL/models\n"]}],"source":["%cd /workspace/GitHub/AVSL/models"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import detectron2.utils.comm as comm\n","from detectron2.config import get_cfg\n","from detectron2.projects.deeplab import add_deeplab_config\n","from detectron2.utils.logger import setup_logger\n","\n","# from mask2former.config import add_maskformer2_config\n","from Mask2former.mask2former import add_maskformer2_config"]},{"cell_type":"markdown","metadata":{},"source":["- init.py: from . import modeling\n","- modeling/init.py: from .pixel_decoder.msdeformattn import MSDeformAttnPixelDecoder\n","- modeling/pixel_decoder/msdeformattn.py: from .ops.modules import MSDeformAttn\n","- modeling/pixel_decoder/ops/modules/ms_deform_attn.py: from ..functions import MSDeformAttnFunction\n","- modeling/pixel_decoder/ops/functions/ms_deform_attn_func.py: import MultiScaleDeformableAttention as MSDA 주석 처리"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["<Logger mask2former (DEBUG)>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Create configs and perform basic setups\n","\n","cfg = get_cfg()\n","# for poly lr schedule\n","add_deeplab_config(cfg)\n","add_maskformer2_config(cfg)\n","cfg.merge_from_file(\"Mask2former/configs/custom/MaskAVSL_swin_large.yaml\")\n","# cfg.merge_from_list(args.opts)\n","cfg.MODEL.DEVICE = \"cuda:1\"\n","cfg.eval_only = True\n","cfg.freeze()\n","# default_setup(cfg, args)\n","# Setup logger for \"mask_former\" module\n","setup_logger(output=cfg.OUTPUT_DIR, distributed_rank=comm.get_rank(), name=\"mask2former\")"]},{"cell_type":"markdown","metadata":{},"source":["# Custom Dataset"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import sys, os\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","from moviepy.editor import VideoFileClip\n","\n","from detectron2.config import configurable\n","from detectron2.data import transforms as T\n","from detectron2.projects.point_rend import ColorAugSSDTransform\n","from detectron2.data import detection_utils as utils"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class VideoDataset(Dataset):\n","    @configurable\n","    def __init__(self, is_train=True, *, augmentations, image_format, ignore_label, size_divisibility, folder_path, ):\n","        self.folder_path = folder_path\n","        self.video_list = [f for f in os.listdir(folder_path) if f.endswith('.mp4')]\n","        self.tfm_gens = augmentations\n","\n","        self.is_train = is_train\n","        self.img_format = image_format\n","        self.ignore_label = ignore_label\n","        self.size_divisibility = size_divisibility\n","\n","    def __len__(self):\n","        return len(self.video_list)\n","\n","    def __getitem__(self, idx):\n","        video_path = os.path.join(self.folder_path, self.video_list[idx])\n","\n","        # Load video and extract central frame\n","        video_clip = VideoFileClip(video_path)\n","        central_frame = video_clip.get_frame(video_clip.duration / 2)\n","        central_frame = np.array(central_frame)\n","        # Convert image frame to numpy array and normalize\n","        #central_frame = central_frame / 255.0\n","\n","        # Additional augmentation\n","        aug_input = T.AugInput(central_frame)\n","        aug_input, transforms = T.apply_transform_gens(self.tfm_gens, aug_input)\n","        central_frame = aug_input.image\n","        #sem_seg_gt = aug_input.sem_seg\n","\n","        central_frame = torch.as_tensor(np.ascontiguousarray(central_frame.transpose(2, 0, 1)))\n","        #central_frame = np.transpose(central_frame, (2, 0, 1))  # Change HWC to CHW\n","\n","        if self.size_divisibility > 0:\n","            central_frame_size = (central_frame.shape[-2], central_frame.shape[-1])\n","            padding_size = [\n","                0,\n","                self.size_divisibility - central_frame_size[1],\n","                0,\n","                self.size_divisibility - central_frame_size[0],\n","            ]\n","            central_frame = F.pad(central_frame, padding_size, value=128).contiguous()\n","\n","        sample = {}\n","        # sample['image'] = central_frame\n","\n","        # Save audio as WAV file\n","        audio_path = f\"{video_path[:-4]}.wav\"\n","        self.blockPrint()\n","        # audio = \n","        video_clip.audio.write_audiofile(audio_path, codec='pcm_s16le', fps=44100)\n","        self.enablePrint()\n","\n","        # Return data as dictionary\n","        sample = {'image': central_frame, 'audio_path': audio_path}\n","        # sample = {'image': central_frame, 'audio': audio}\n","\n","        return sample\n","\n","    @classmethod\n","    def from_config(cls, cfg, is_train=True):\n","        # Build augmentation\n","        augs = [\n","            T.ResizeShortestEdge(\n","                cfg.INPUT.MIN_SIZE_TRAIN,\n","                cfg.INPUT.MAX_SIZE_TRAIN,\n","                cfg.INPUT.MIN_SIZE_TRAIN_SAMPLING,\n","            )\n","        ]\n","        if cfg.INPUT.CROP.ENABLED:\n","            augs.append(\n","                T.RandomCrop_CategoryAreaConstraint(\n","                    cfg.INPUT.CROP.TYPE,\n","                    cfg.INPUT.CROP.SIZE,\n","                    cfg.INPUT.CROP.SINGLE_CATEGORY_MAX_AREA,\n","                    cfg.MODEL.SEM_SEG_HEAD.IGNORE_VALUE,\n","                )\n","            )\n","        if cfg.INPUT.COLOR_AUG_SSD:\n","            augs.append(ColorAugSSDTransform(img_format=cfg.INPUT.FORMAT))\n","        augs.append(T.RandomFlip())\n","        augs.extend([\n","        T.ResizeScale(\n","            min_scale=cfg.INPUT.MIN_SCALE, max_scale=cfg.INPUT.MAX_SCALE,\n","            target_height=cfg.INPUT.IMAGE_SIZE, target_width=cfg.INPUT.IMAGE_SIZE\n","        ),\n","        T.FixedSizeCrop(crop_size=(cfg.INPUT.IMAGE_SIZE, cfg.INPUT.IMAGE_SIZE)),\n","        ])\n","\n","        ignore_label = False\n","\n","        ret = {\n","            \"is_train\": is_train,\n","            \"augmentations\": augs,\n","            \"image_format\": cfg.INPUT.FORMAT,\n","            \"ignore_label\": ignore_label,\n","            \"size_divisibility\": cfg.INPUT.SIZE_DIVISIBILITY,\n","        }\n","        return ret\n","    \n","    def blockPrint(self):\n","        global backupstdout\n","        backupstdout=sys.stdout\n","        sys.stdout = open(os.devnull, 'w')\n","\n","    def enablePrint(self):\n","        global backupstdout\n","        sys.stdout = backupstdout"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["folder_path = '../DATA/videos'\n","video_dataset = VideoDataset(cfg, folder_path=folder_path)\n","dataloader = DataLoader(video_dataset, batch_size=cfg.SOLVER.IMS_PER_BATCH, shuffle=True)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["23442\n","1466\n"]}],"source":["print(len(video_dataset))\n","print(len(dataloader))"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                        \r"]}],"source":["# detectron2/engine/train_loop.py\n","\n","data = next(iter(dataloader))"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 16/16 [00:00<00:00, 107.33it/s]\n"]}],"source":["# maskformer_model.py\n","from detectron2.structures import ImageList\n","from tqdm import tqdm\n","\n","pixel_mean = cfg.MODEL.PIXEL_MEAN\n","pixel_std = cfg.MODEL.PIXEL_STD\n","size_divisibility = cfg.MODEL.MASK_FORMER.SIZE_DIVISIBILITY\n","device = cfg.MODEL.DEVICE\n","\n","pixel_mean = torch.Tensor(pixel_mean).view(-1, 1, 1)\n","pixel_std = torch.Tensor(pixel_std).view(-1, 1, 1)\n","\n","# images = [x[\"image\"].to(device) for x in tqdm(data)]\n","images = [x for x in data['image']]\n","images = [(x - pixel_mean) / pixel_std for x in tqdm(images)]\n","images = ImageList.from_tensors(images, size_divisibility)"]},{"cell_type":"markdown","metadata":{},"source":["### show"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["# images = []\n","# for i, x in tqdm(enumerate(dataloader)): \n","#     img = x[\"image\"]\n","#     img = (img - pixel_mean) / pixel_std\n","#     images.append(img)\n","#     if i == 10:\n","#         break # 11개에 2분/이었는데 6분으로 늘어남 \n","# images = ImageList.from_tensors(images, size_divisibility)\n","# images.to(device)"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["# detectron2/modeling/meta_arch/build.py\n","# def build_model(cfg):\n","#     \"\"\"\n","#     Build the whole model architecture, defined by ``cfg.MODEL.META_ARCHITECTURE``.\n","#     Note that it does not load any weights from ``cfg``.\n","#     \"\"\"\n","#     meta_arch = cfg.MODEL.META_ARCHITECTURE\n","#     model = META_ARCH_REGISTRY.get(meta_arch)(cfg) # 'MaskFormer'\n","#     model.to(torch.device(cfg.MODEL.DEVICE))\n","#     _log_api_usage(\"modeling.meta_arch.\" + meta_arch)\n","#     return model\n","# -> model = maskformer_model.py\n","\n","# detectron2/engine/defaults.py\n","# data_loader = self.build_train_loader(cfg) -> torchdata.DataLoader(dataset, batch_size=batch_size,)\n","# _trainer = (AMPTrainer if cfg.SOLVER.AMP.ENABLED else SimpleTrainer)(model, data_loader, optimizer)\n","\n","# detectron2/engine/train_loop.py\n","# def _data_loader_iter(self):\n","#     # only create the data loader iterator when it is used\n","#     if self._data_loader_iter_obj is None:\n","#         self._data_loader_iter_obj = iter(self.data_loader)\n","#     return self._data_loader_iter_obj\n","# data = next(self._data_loader_iter) -> next(iter(self.data_loader)) \n","# model(data)"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["# example = video_dataset.__getitem__(1)\n","# print(example.keys())"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["# example['image'].shape"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["# example['audio_path']"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["# from matplotlib import pyplot as plt\n","# plt.imshow(example['image'].permute(1, 2, 0), interpolation='nearest')\n","# plt.show()"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["# from scipy.io import wavfile\n","\n","# fs, audio_data = wavfile.read(example['audio_path'])\n","# print(fs, audio_data.shape)\n","\n","# # plt.figure(figsize = (12, 3))\n","# plt.plot(audio_data, lw = 1)\n","# plt.xlim(0, len(audio_data))"]},{"cell_type":"markdown","metadata":{"id":"Sm5D6upQUAoy"},"source":["# Model Architecture"]},{"cell_type":"markdown","metadata":{},"source":["## backbone"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/workspace/GitHub/AVSL/models\n"]}],"source":["%cd /workspace/GitHub/AVSL/models"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["# from detectron2.modeling import build_backbone\n","\n","# backbone = build_backbone(cfg)\n","# backbone = BACKBONE_REGISTRY.get(backbone_name)(cfg, input_shape)"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["# from detectron2.layers import ShapeSpec\n","# from Mask2former.mask2former.modeling.backbone.swin import D2SwinTransformer\n","\n","# print(cfg.MODEL.WEIGHTS)\n","# input_shape = ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN))\n","# backbone = D2SwinTransformer(cfg, input_shape)\n","# backbone.init_weights(cfg.MODEL.WEIGHTS)\n","# # backbone.to(device)\n","\n","# features = backbone(images.tensor)"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["# model.py\n","from detectron2.layers import ShapeSpec\n","from Mask2former.mask2former.modeling.backbone.swin import D2SwinTransformer\n","\n","def build_swin_backbone(cfg):\n","    input_shape = ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN))\n","    model = D2SwinTransformer(cfg, input_shape)\n","    model.init_weights(cfg.MODEL.WEIGHTS)\n","    return model\n","backbone = build_swin_backbone(cfg)\n","image_feature = backbone(images.tensor)"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[{"data":{"text/plain":["dict_keys(['res2', 'res3', 'res4', 'res5'])"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["image_feature.keys()"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([16, 192, 256, 256])\n","torch.Size([16, 384, 128, 128])\n","torch.Size([16, 768, 64, 64])\n","torch.Size([16, 1536, 32, 32])\n"]}],"source":["print(image_feature['res2'].shape)\n","print(image_feature['res3'].shape)\n","print(image_feature['res4'].shape)\n","print(image_feature['res5'].shape)"]},{"cell_type":"markdown","metadata":{},"source":["## pixel_decoder"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["# from detectron2.modeling import build_sem_seg_head\n","\n","# sem_seg_head = build_sem_seg_head(cfg, backbone.output_shape())\n","# -> SEM_SEG_HEADS_REGISTRY.get(name)(cfg, input_shape)"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":["# from mask2former.maskformer_model.modeling.pixel_decoder.fpn import build_pixel_decoder\n","\n","# build_pixel_decoder(cfg, input_shape)\n","# -> model = SEM_SEG_HEADS_REGISTRY.get(name)(cfg, input_shape)"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[{"data":{"text/plain":["{'res2': ShapeSpec(channels=192, height=None, width=None, stride=4),\n"," 'res3': ShapeSpec(channels=384, height=None, width=None, stride=8),\n"," 'res4': ShapeSpec(channels=768, height=None, width=None, stride=16),\n"," 'res5': ShapeSpec(channels=1536, height=None, width=None, stride=32)}"]},"execution_count":71,"metadata":{},"output_type":"execute_result"}],"source":["backbone.output_shape()"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[],"source":["# from mask2former.modeling.pixel_decoder.fpn import TransformerEncoderPixelDecoder\n","\n","# pixel_decoder = TransformerEncoderPixelDecoder(cfg, input_shape=backbone.output_shape()) # maskformer_model.py\n","\n","# # mask_foremer_head.py\n","# pp_embeds, transformer_encoder_features, _ = pixel_decoder.forward_features(features)"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Calling forward() may cause unpredicted behavior of PixelDecoder module.\n"]}],"source":["# model.py\n","from Mask2former.mask2former.modeling.pixel_decoder.fpn import TransformerEncoderPixelDecoder\n","\n","def build_pixel_decoder(cfg, input_shape):\n","    model = TransformerEncoderPixelDecoder(cfg, input_shape)\n","    return model\n","\n","pixel_decoder = build_pixel_decoder(cfg, backbone.output_shape())\n","pp_embeds, image_features, _ = pixel_decoder(image_feature)"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([16, 256, 32, 32])"]},"execution_count":74,"metadata":{},"output_type":"execute_result"}],"source":["image_features.shape"]},{"cell_type":"markdown","metadata":{},"source":["## transformer decoder"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":["# # mask_former_head.py\n","# if cfg.MODEL.MASK_FORMER.TRANSFORMER_IN_FEATURE == \"transformer_encoder\":\n","#     transformer_predictor_in_channels = cfg.MODEL.SEM_SEG_HEAD.CONVS_DIM\n","# elif cfg.MODEL.MASK_FORMER.TRANSFORMER_IN_FEATURE == \"pixel_embedding\":\n","#     transformer_predictor_in_channels = cfg.MODEL.SEM_SEG_HEAD.MASK_DIM\n","# elif cfg.MODEL.MASK_FORMER.TRANSFORMER_IN_FEATURE == \"multi_scale_pixel_decoder\":  # for maskformer2\n","#     transformer_predictor_in_channels = cfg.MODEL.SEM_SEG_HEAD.CONVS_DIM\n","# else:\n","#     transformer_predictor_in_channels = input_shape[cfg.MODEL.MASK_FORMER.TRANSFORMER_IN_FEATURE].channels\n","\n","# # \"transformer_predictor\": build_transformer_decoder(cfg, transformer_predictor_in_channels, mask_classification=True,)\n","# # -> TRANSFORMER_DECODER_REGISTRY.get(name)(cfg, in_channels, mask_classification)"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[],"source":["# from mask2former.modeling.transformer_decoder.maskformer_transformer_decoder import StandardTransformerDecoder\n","\n","# transformer_decoder = StandardTransformerDecoder(cfg, in_channels=transformer_predictor_in_channels, mask_classification=False)"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[],"source":["# mask_former_head.py\n","# predictions = transformer_decoder(transformer_encoder_features, mask_features, mask=None) # mask에 뭐가 들어가야 하는지 모르겠다"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[],"source":["# predictions.keys()"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[],"source":["# predictions['pred_masks'].shape # scoremap\n","# # predictions['pred_masks']"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[],"source":["# print(predictions['pred_masks'].min())\n","# print(predictions['pred_masks'].max())\n","# # 음수 값이 나오는 것을 방지하기 위해 sigmoid 함수 고려"]},{"cell_type":"markdown","metadata":{},"source":["## transformer decoder 수정"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[],"source":["# maskformer_transformer_decoder_custom.py\n","from Mask2former.mask2former.modeling.transformer_decoder.transformer import Transformer\n","from Mask2former.mask2former.modeling.transformer_decoder.position_encoding import PositionEmbeddingSine\n","from torch import nn\n","\n","N_steps = cfg.MODEL.MASK_FORMER.HIDDEN_DIM // 2\n","pe_layer = PositionEmbeddingSine(N_steps, normalize=True)\n","\n","batch_size=cfg.SOLVER.IMS_PER_BATCH\n","\n","num_queries = cfg.MODEL.MASK_FORMER.NUM_OBJECT_QUERIES\n","transformer = Transformer(\n","            d_model=cfg.MODEL.MASK_FORMER.HIDDEN_DIM,\n","            dropout=cfg.MODEL.MASK_FORMER.DROPOUT,\n","            nhead=cfg.MODEL.MASK_FORMER.NHEADS,\n","            dim_feedforward=cfg.MODEL.MASK_FORMER.DIM_FEEDFORWARD,\n","            num_encoder_layers=cfg.MODEL.MASK_FORMER.ENC_LAYERS,\n","            num_decoder_layers=cfg.MODEL.MASK_FORMER.DEC_LAYERS,\n","            normalize_before=cfg.MODEL.MASK_FORMER.PRE_NORM,\n","            return_intermediate_dec=cfg.MODEL.MASK_FORMER.DEEP_SUPERVISION,\n","        )\n","hidden_dim = transformer.d_model\n","\n","\n","if cfg.MODEL.MASK_FORMER.TRANSFORMER_IN_FEATURE == \"transformer_encoder\":\n","    transformer_predictor_in_channels = cfg.MODEL.SEM_SEG_HEAD.CONVS_DIM\n","elif cfg.MODEL.MASK_FORMER.TRANSFORMER_IN_FEATURE == \"pixel_embedding\":\n","    transformer_predictor_in_channels = cfg.MODEL.SEM_SEG_HEAD.MASK_DIM"]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[],"source":["import fvcore.nn.weight_init as weight_init\n","from detectron2.layers import Conv2d\n","from Mask2former.mask2former.modeling.transformer_decoder.maskformer_transformer_decoder_custom import MLP\n","\n","learnable_input_queries = nn.Embedding(num_queries, hidden_dim)\n","\n","in_channels = transformer_predictor_in_channels # mask_former_head.py\n","enforce_input_project = cfg.MODEL.MASK_FORMER.ENFORCE_INPUT_PROJ\n","deep_supervision = cfg.MODEL.MASK_FORMER.DEEP_SUPERVISION\n","mask_dim = cfg.MODEL.SEM_SEG_HEAD.MASK_DIM\n","\n","if in_channels != hidden_dim or enforce_input_project:\n","    input_proj = Conv2d(in_channels, hidden_dim, kernel_size=1)\n","    weight_init.c2_xavier_fill(input_proj)\n","else:\n","    input_proj = nn.Sequential()\n","aux_loss = deep_supervision\n","\n","mask_embed = MLP(hidden_dim, hidden_dim, mask_dim, 3)"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[],"source":["x = image_features"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["mask = None\n","\n","pos = pe_layer(x, mask)\n","\n","src = x\n","hs, memory = transformer(input_proj(src), mask, learnable_input_queries.weight, pos)"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([16, 100, 128])\n"]}],"source":["# output_size = hs.shape\n","\n","audio_separation_tokens = hs[-1]\n","token_size = hs[-1].shape\n","print(token_size)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### after audio"]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[],"source":["# def audio_separation_model(mixed_audio, audio_separation_token, noise_token): # mixed_audio: [batch_size, _, _]\n","#     query_input = torch.cat([audio_separation_token, noise_token], dim=1) # [batch_size, num_queries+1, 128]\n","#     separated_audio_wav_list = []\n","#     # 이 아래는 바꿔야 함\n","#     for i in range(num_queries):\n","#         separated_audio_wav_list.append(torch.rand(batch_size, 1, 903, hidden_dim))\n","#     return mixed_audio, separated_audio_wav_list\n","\n","# mixed_audio, separated_audio_wav_list = audio_separation_model(None, audio_separation_tokens, noise_token)"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[],"source":["# def audio_encoder(separated_audio_wav_list):\n","#     separated_audio_wav_features_list = []\n","#     for wav in separated_audio_wav_list:\n","#             out = torch.rand(batch_size, 903, hidden_dim) # num_queries = len(separated_audio_wav_features_list)\n","#             separated_audio_wav_features_list.append(out)\n","#     return separated_audio_wav_features_list\n","\n","# separated_audio_wav_features_list = audio_encoder(separated_audio_wav_list)\n","# len(separated_audio_wav_features_list)"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[],"source":["# def av_feature_embed(separated_audio_wav_features_list, pooling_mode='sum'):\n","#     av_feature_list = ImageList.from_tensors(separated_audio_wav_features_list).tensor\n","#     av_feature_list = av_feature_list.permute(1, 0, 2, 3)\n","#     if pooling_mode == 'sum':\n","#         av_feature_list = av_feature_list.sum(dim=2)\n","#     elif pooling_mode == 'mean':\n","#         av_feature_list = av_feature_list.mean(dim=2)\n","#     elif pooling_mode == 'max':\n","#         av_feature_list = av_feature_list.max(dim=2).values\n","#     return av_feature_list\n","\n","# av_feature_list = av_feature_embed(separated_audio_wav_features_list, pooling_mode='sum')\n","# av_feature_list.shape"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[],"source":["# def audio_module(mixed_audio, query_output, noise_query, batch_size):\n","#     query_input = torch.cat([query_output, noise_query], dim=1)\n","#     out = torch.rand(batch_size, num_queries, 903, hidden_dim)\n","#     return out\n","# av_feature = audio_module(None, hs[-1], noise_query, batch_size) # torch.Size([16, 100, 128])"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'av_feature_list' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_785262/1030258379.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mav_feature_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mav_feature_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'av_feature_list' is not defined"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["# av_feature_embed = mask_embed(av_feature_list)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([16, 100, 256])"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["# av_feature_embed.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([16, 100, 256, 256])"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["# scoremap = torch.einsum(\"bqc,bchw->bqhw\", av_feature_embed, mask_features)\n","# scoremap.shape\n","# scoremap = torch.einsum(\"lbqc,bchw->lbqhw\", av_feature_embed, mask_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1706449382705,"user":{"displayName":"구은아","userId":"03420424854508517452"},"user_tz":-540},"id":"9i6cJdI2D2AN","outputId":"b7fe9b2f-a8bd-4f2f-be59-567130d04da0"},"outputs":[],"source":["# from mask2former.modeling.transformer_decoder.maskformer_transformer_decoder_custom import StandardTransformerDecoder\n","\n","# def build_transformer_decoder(cfg):\n","#     if cfg.MODEL.MASK_FORMER.TRANSFORMER_IN_FEATURE == \"transformer_encoder\":\n","#         transformer_predictor_in_channels = cfg.MODEL.SEM_SEG_HEAD.CONVS_DIM\n","#     elif cfg.MODEL.MASK_FORMER.TRANSFORMER_IN_FEATURE == \"pixel_embedding\":\n","#         transformer_predictor_in_channels = cfg.MODEL.SEM_SEG_HEAD.MASK_DIM\n","#     elif cfg.MODEL.MASK_FORMER.TRANSFORMER_IN_FEATURE == \"multi_scale_pixel_decoder\":  # for maskformer2\n","#         transformer_predictor_in_channels = cfg.MODEL.SEM_SEG_HEAD.CONVS_DIM\n","#     model = StandardTransformerDecoder(cfg, in_channels=transformer_predictor_in_channels, mask_classification=False)\n","#     return model\n","\n","# transformer_decoder = build_transformer_decoder(cfg)\n","# outputs = transformer_decoder(transformer_encoder_features, mask_features, mixed_audio=None, mask=None)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["dict_keys(['pred_masks'])"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["# outputs.keys()"]},{"cell_type":"markdown","metadata":{},"source":["# LAAS"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/workspace/GitHub/AVSL/models\n"]}],"source":["%cd /workspace/GitHub/AVSL/models"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([16, 160640, 1])"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["import librosa\n","\n","audio_list = []\n","for path in data['audio_path']:\n","    y, sr = librosa.load(path, mono=True, sr=16000)\n","    audio_list.append(torch.from_numpy(y).unsqueeze(1))\n","\n","y = ImageList.from_tensors(audio_list).tensor\n","y.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([16, 100, 128])"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["audio_separation_tokens.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([16, 101, 128])"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["noise_token = torch.rand(token_size[0], 1, token_size[2])\n","input_tokens = torch.cat([audio_separation_tokens, noise_token], dim=1)\n","input_tokens.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# x = {'audio':torch.from_numpy(y.reshape((2,y.shape[1],1))),\n","#      'conditions':torch.from_numpy(np.zeros((2,9,128),dtype=np.float32))} # (batch_size, 오디오길이, 1), mono로 바꾸어야 함\n","\n","input = {\"audio\": y, \"conditions\": input_tokens}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["from AudioModule.LAAS import LAAS\n","from AudioModule import config\n","\n","laas = LAAS(config=config)\n","# laas(input) # kernel crashed"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["16"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["input[\"conditions\"].shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["# pytorch-lightning: pip install pytorch-lightning==1.4.4, pip install torchmetrics==0.6.0\n","# from AudioModule.LAAS import ZeroShotASP\n","# from AudioModule import config\n","\n","# ASP = ZeroShotASP(config=config, channels=input[\"conditions\"].shape[1])\n","\n","# x = ASP(input=input[\"audio\"], condition=input[\"conditions\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2599,"status":"ok","timestamp":1706449387852,"user":{"displayName":"구은아","userId":"03420424854508517452"},"user_tz":-540},"id":"xBUymms27gMA"},"outputs":[],"source":["mask_pred_results = F.interpolate(\n","                mask_pred_results,\n","                size=(images.tensor.shape[-2], images.tensor.shape[-1]),\n","                mode=\"bilinear\",\n","                align_corners=False,\n","            )"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1706449430235,"user":{"displayName":"구은아","userId":"03420424854508517452"},"user_tz":-540},"id":"s__BZNbIT5i0"},"outputs":[],"source":["image_size = images.image_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":371,"status":"ok","timestamp":1706450905743,"user":{"displayName":"구은아","userId":"03420424854508517452"},"user_tz":-540},"id":"s9AH6HgVVBNT"},"outputs":[],"source":["def semantic_inference(mask_cls, mask_pred):\n","    mask_cls = F.softmax(mask_cls, dim=-1)[..., :-1]\n","    mask_pred = mask_pred.sigmoid()\n","    semseg = torch.einsum(\"qc,qhw->chw\", mask_cls, mask_pred)\n","    return semseg\n","\n","def panoptic_inference(mask_cls, mask_pred):\n","    scores, labels = F.softmax(mask_cls, dim=-1).max(-1)\n","    mask_pred = mask_pred.sigmoid()\n","\n","    keep = labels.ne(sem_seg_head.num_classes) & (scores > object_mask_threshold)\n","    cur_scores = scores[keep]\n","    cur_classes = labels[keep]\n","    cur_masks = mask_pred[keep]\n","    cur_mask_cls = mask_cls[keep]\n","    cur_mask_cls = cur_mask_cls[:, :-1]\n","\n","    cur_prob_masks = cur_scores.view(-1, 1, 1) * cur_masks\n","\n","    h, w = cur_masks.shape[-2:]\n","    panoptic_seg = torch.zeros((h, w), dtype=torch.int32, device=cur_masks.device)\n","    segments_info = []\n","\n","    current_segment_id = 0\n","\n","    if cur_masks.shape[0] == 0:\n","        # We didn't detect any mask :(\n","        return panoptic_seg, segments_info\n","    else:\n","        # take argmax\n","        cur_mask_ids = cur_prob_masks.argmax(0)\n","        stuff_memory_list = {}\n","        for k in range(cur_classes.shape[0]):\n","            pred_class = cur_classes[k].item()\n","            isthing = pred_class in metadata.thing_dataset_id_to_contiguous_id.values()\n","            mask_area = (cur_mask_ids == k).sum().item()\n","            original_area = (cur_masks[k] >= 0.5).sum().item()\n","            mask = (cur_mask_ids == k) & (cur_masks[k] >= 0.5)\n","\n","            if mask_area > 0 and original_area > 0 and mask.sum().item() > 0:\n","                if mask_area / original_area < overlap_threshold:\n","                    continue\n","\n","                # merge stuff regions\n","                if not isthing:\n","                    if int(pred_class) in stuff_memory_list.keys():\n","                        panoptic_seg[mask] = stuff_memory_list[int(pred_class)]\n","                        continue\n","                    else:\n","                        stuff_memory_list[int(pred_class)] = current_segment_id + 1\n","\n","                current_segment_id += 1\n","                panoptic_seg[mask] = current_segment_id\n","\n","                segments_info.append(\n","                    {\n","                        \"id\": current_segment_id,\n","                        \"isthing\": bool(isthing),\n","                        \"category_id\": int(pred_class),\n","                    }\n","                )\n","\n","        return panoptic_seg, segments_info\n","\n","def instance_inference(mask_cls, mask_pred):\n","    # mask_pred is already processed to have the same shape as original input\n","    image_size = mask_pred.shape[-2:]\n","\n","    # [Q, K]\n","    scores = F.softmax(mask_cls, dim=-1)[:, :-1]\n","    labels = torch.arange(sem_seg_head.num_classes, device=device).unsqueeze(0).repeat(num_queries, 1).flatten(0, 1)\n","    # scores_per_image, topk_indices = scores.flatten(0, 1).topk(num_queries, sorted=False)\n","    scores_per_image, topk_indices = scores.flatten(0, 1).topk(test_topk_per_image, sorted=False)\n","    labels_per_image = labels[topk_indices]\n","\n","    topk_indices = topk_indices // sem_seg_head.num_classes\n","    # mask_pred = mask_pred.unsqueeze(1).repeat(1, sem_seg_head.num_classes, 1).flatten(0, 1)\n","    mask_pred = mask_pred[topk_indices]\n","\n","    # if this is panoptic segmentation, we only keep the \"thing\" classes\n","    if panoptic_on:\n","        keep = torch.zeros_like(scores_per_image).bool()\n","        for i, lab in enumerate(labels_per_image):\n","            keep[i] = lab in metadata.thing_dataset_id_to_contiguous_id.values()\n","\n","        scores_per_image = scores_per_image[keep]\n","        labels_per_image = labels_per_image[keep]\n","        mask_pred = mask_pred[keep]\n","\n","    result = Instances(image_size)\n","    # mask (before sigmoid)\n","    result.pred_masks = (mask_pred > 0).float()\n","    result.pred_boxes = Boxes(torch.zeros(mask_pred.size(0), 4))\n","    # Uncomment the following to get boxes from masks (this is slow)\n","    # result.pred_boxes = BitMasks(mask_pred > 0).get_bounding_boxes()\n","\n","    # calculate average mask prob\n","    mask_scores_per_image = (mask_pred.sigmoid().flatten(1) * result.pred_masks.flatten(1)).sum(1) / (result.pred_masks.flatten(1).sum(1) + 1e-6)\n","    result.scores = scores_per_image * mask_scores_per_image\n","    result.pred_classes = labels_per_image\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8557,"status":"ok","timestamp":1706450933103,"user":{"displayName":"구은아","userId":"03420424854508517452"},"user_tz":-540},"id":"HENIYgLYTzxi"},"outputs":[],"source":["processed_results = []\n","batched_inputs = dataset_dict[\"image\"]\n","\n","for mask_cls_result, mask_pred_result, input_per_image, image_size in zip(\n","    mask_cls_results, mask_pred_results, batched_inputs, images.image_sizes):\n","    height = 1024 # input_per_image.get(\"height\", image_size[0])\n","    width = 1024 # input_per_image.get(\"width\", image_size[1])\n","    processed_results.append({})\n","\n","    if sem_seg_postprocess_before_inference:\n","        mask_pred_result = retry_if_cuda_oom(sem_seg_postprocess)(\n","            mask_pred_result, image_size, height, width\n","        )\n","        mask_cls_result = mask_cls_result.to(mask_pred_result)\n","\n","    # semantic segmentation inference\n","    if semantic_on:\n","        r = retry_if_cuda_oom(semantic_inference)(mask_cls_result, mask_pred_result)\n","        if not sem_seg_postprocess_before_inference:\n","            r = retry_if_cuda_oom(sem_seg_postprocess)(r, image_size, height, width)\n","        processed_results[-1][\"sem_seg\"] = r\n","\n","    # panoptic segmentation inference\n","    if panoptic_on:\n","        panoptic_r = retry_if_cuda_oom(panoptic_inference)(mask_cls_result, mask_pred_result)\n","        processed_results[-1][\"panoptic_seg\"] = panoptic_r\n","\n","    # instance segmentation inference\n","    if instance_on:\n","        instance_r = retry_if_cuda_oom(instance_inference)(mask_cls_result, mask_pred_result)\n","        processed_results[-1][\"instances\"] = instance_r\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1706451124981,"user":{"displayName":"구은아","userId":"03420424854508517452"},"user_tz":-540},"id":"9rbBuF-4TzuD","outputId":"afcdb8f5-41e4-4c5f-cac9-8fb686e58f59"},"outputs":[{"data":{"text/plain":["dict_keys(['sem_seg', 'panoptic_seg', 'instances'])"]},"execution_count":72,"metadata":{},"output_type":"execute_result"}],"source":["processed_results[0].keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1706451149416,"user":{"displayName":"구은아","userId":"03420424854508517452"},"user_tz":-540},"id":"WnJ-7xzQTzqb","outputId":"92856944-a6ca-4979-eea0-e98dbc5fc447"},"outputs":[{"data":{"text/plain":["torch.Size([133, 1024, 1024])"]},"execution_count":74,"metadata":{},"output_type":"execute_result"}],"source":["processed_results[0]['sem_seg'].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-EJulqDDTzGa"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["pDe-87-15hMh"],"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"}},"nbformat":4,"nbformat_minor":0}
