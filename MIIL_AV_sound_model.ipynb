{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cid2rrrr/AVSL/blob/main/MIIL_AV_sound_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Prk6F156CUw6"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLclkKp-LUXu"
      },
      "source": [
        "**Idea = Large scale training (without GT)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVDcr3InCZda"
      },
      "source": [
        "**Youtube video dataset**\n",
        "\n",
        "ACAV100M largest audio-visual dataset (2023)\n",
        "\n",
        "<img src = \"./IMGs/Dataset.png\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zttO8CEkNuru"
      },
      "source": [
        "**TEST Dataset**\n",
        "\n",
        "- URMP Dataset\n",
        "  - 44 vids\n",
        "  - 14 kinds of inst.\n",
        "  - Recorded w/ Chroma-key background (solo)\n",
        "  - merge vids & add background (make duet..)\n",
        "\n",
        "- ~~MUSIC Dataset~~\n",
        "  - 임시 보류\n",
        "\n",
        "- Additional Dataset needed....\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoSAcjNGRoxn"
      },
      "source": [
        "# Pipeline\n",
        "\n",
        "<img src = \"./IMGs/Model_Config.png\">\n",
        "\n",
        "**Contributions**\n",
        "\n",
        "1. Audio Separation model\n",
        "\n",
        "2. Silent Audio Filtering Network\n",
        "\n",
        "3. Mask Evaluation Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kj6tTy25CYTp"
      },
      "source": [
        "# Sound Separation Model(U-net, Transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2VNJoWCLjS9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QGw0zqvvHkS"
      },
      "source": [
        "- D3Net\n",
        "  - https://arxiv.org/pdf/2011.11844.pdf\n",
        "- Demucs\n",
        "  - https://arxiv.org/pdf/1909.01174.pdf\n",
        "- Meta-TasNet\n",
        "  - https://arxiv.org/pdf/2002.07016.pdf\n",
        "- DPRNN\n",
        "  - https://arxiv.org/pdf/1910.06379.pdf\n",
        "- Voice Separation with an Unknown Number of Multiple Speakers\n",
        "  - https://arxiv.org/pdf/2003.01531.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddKURDH2OHQe"
      },
      "source": [
        "# Audio Filtering Network(spectral flattness)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3c1RG1QOK4m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdz_MQwfDB7X"
      },
      "source": [
        "# Audio Encoder (Frozen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwnHkWH3LmQ1"
      },
      "outputs": [],
      "source": [
        "# VGGish encoder\n",
        "__copyright__ = \"Copyright (c) 2021 Jina AI Limited. All rights reserved.\"\n",
        "__license__ = \"Apache-2.0\"\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "import requests as _requests\n",
        "import tensorflow as tf\n",
        "from jina import DocumentArray, Executor, requests\n",
        "from jina.logging.logger import JinaLogger\n",
        "\n",
        "from .vggish.vggish_params import INPUT_TENSOR_NAME, OUTPUT_TENSOR_NAME\n",
        "from .vggish.vggish_postprocess import Postprocessor\n",
        "from .vggish.vggish_slim import define_vggish_slim, load_vggish_slim_checkpoint\n",
        "from .vggish.vggish_input import wavfile_to_examples, mp3file_to_examples, waveform_to_examples\n",
        "\n",
        "import warnings\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "cur_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "\n",
        "\n",
        "class VggishAudioEncoder(Executor):\n",
        "    \"\"\"\n",
        "    Encode audio data with Vggish embeddings\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path: str = Path(cur_dir).parents[0] / 'models',\n",
        "        load_input_from: str = 'uri',\n",
        "        min_duration: int = 10,\n",
        "        device: str = '/CPU:0',\n",
        "        access_paths: str = '@r',\n",
        "        traversal_paths: Optional[str] = None,\n",
        "        batch_size: int = 32,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param model_path: path of the models directory. The directory should contain\n",
        "            'vggish_model.ckpt' and 'vggish_pca_params.ckpt'. Setting this to a new directory\n",
        "            will download the files.\n",
        "        :param load_input_from: the place where input data is stored, either 'uri', 'log_mel', 'waveform'.\n",
        "            When set to 'uri', the model reads wave file (.mp3 or .wav) from the file path specified by the 'uri'.\n",
        "            When set to 'log_mel', the model reads log melspectrogram array from the `blob` attribute.\n",
        "            When set to 'waveform', the model reads wave form array from the `blob` attribute. This requires the sample rate information stored at `.tags['sample_rate']` as `float`.\n",
        "        :param min_duration: the minimal duration of the audio data in seconds. The input data will not be encoded if it is shorter than this duration.\n",
        "        :param device: device to run the model on e.g. '/CPU:0','/GPU:0','/GPU:2'\n",
        "        :param batch_size: Default batch size for encoding, used if the\n",
        "            batch size is not passed as a parameter with the request.\n",
        "        :param access_paths: fallback batch size in case there is not\n",
        "            batch size sent in the request\n",
        "        :param traversal_paths: please use access_paths\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__(*args, **kwargs)\n",
        "        if traversal_paths is not None:\n",
        "            self.access_paths = traversal_paths\n",
        "            warnings.warn(\"'traversal_paths' will be deprecated in the future, please use 'access_paths'.\",\n",
        "                          DeprecationWarning,\n",
        "                          stacklevel=2)\n",
        "        else:\n",
        "            self.access_paths = access_paths\n",
        "        self.logger = JinaLogger(self.__class__.__name__)\n",
        "        self.device = device\n",
        "        self.min_duration = min_duration\n",
        "        if load_input_from not in ('uri', 'log_mel', 'waveform'):\n",
        "            self.logger.warning(f'unknown setting to load_input_form. Set to default value, load_input_from=\"uri\"')\n",
        "            load_input_from = 'uri'\n",
        "        self._input = load_input_from\n",
        "        self.model_path = Path(model_path)\n",
        "        self.vgg_model_path = self.model_path / 'vggish_model.ckpt'\n",
        "        self.pca_model_path = self.model_path / 'vggish_pca_params.ckpt'\n",
        "        self.model_path.mkdir(\n",
        "            exist_ok=True\n",
        "        )  # Create the model directory if it does not exist yet\n",
        "\n",
        "        cpus = tf.config.experimental.list_physical_devices(device_type='CPU')\n",
        "        gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
        "        if 'GPU' in device:\n",
        "            gpu_index = 0 if 'GPU:' not in device else int(device.split(':')[-1])\n",
        "            if len(gpus) < gpu_index + 1:\n",
        "                raise RuntimeError(f'Device {device} not found on your system!')\n",
        "            cpus.append(gpus[gpu_index])\n",
        "        tf.config.experimental.set_visible_devices(devices=cpus)\n",
        "\n",
        "        if not self.vgg_model_path.exists():\n",
        "            self.logger.info(\n",
        "                'VGGish model cannot be found from the given model path, '\n",
        "                'downloading a new one...'\n",
        "            )\n",
        "            try:\n",
        "                r = _requests.get(\n",
        "                    'https://storage.googleapis.com/audioset/vggish_model.ckpt'\n",
        "                )\n",
        "                r.raise_for_status()\n",
        "            except _requests.exceptions.HTTPError:\n",
        "                self.logger.error(\n",
        "                    'received HTTP error response, cannot download vggish model'\n",
        "                )\n",
        "                raise\n",
        "            except _requests.exceptions.RequestException:\n",
        "                self.logger.error('Connection error, cannot download vggish model')\n",
        "                raise\n",
        "\n",
        "            with open(self.vgg_model_path, 'wb') as f:\n",
        "                f.write(r.content)\n",
        "\n",
        "        if not self.pca_model_path.exists():\n",
        "            self.logger.info(\n",
        "                'PCA model cannot be found from the given model path, '\n",
        "                'downloading a new one...'\n",
        "            )\n",
        "            try:\n",
        "                r = _requests.get(\n",
        "                    'https://storage.googleapis.com/audioset/vggish_pca_params.npz'\n",
        "                )\n",
        "                r.raise_for_status()\n",
        "            except _requests.exceptions.HTTPError:\n",
        "                self.logger.error(\n",
        "                    'received HTTP error response, cannot download pca model'\n",
        "                )\n",
        "                raise\n",
        "            except _requests.exceptions.RequestException:\n",
        "                self.logger.error('Connection error, cannot download pca model')\n",
        "                raise\n",
        "\n",
        "            with open(self.pca_model_path, 'wb') as f:\n",
        "                f.write(r.content)\n",
        "\n",
        "        self.sess = tf.compat.v1.Session()\n",
        "        define_vggish_slim()\n",
        "        load_vggish_slim_checkpoint(self.sess, str(self.vgg_model_path))\n",
        "        self.feature_tensor = self.sess.graph.get_tensor_by_name(INPUT_TENSOR_NAME)\n",
        "        self.embedding_tensor = self.sess.graph.get_tensor_by_name(OUTPUT_TENSOR_NAME)\n",
        "        self.post_processor = Postprocessor(str(self.pca_model_path))\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    @requests\n",
        "    def encode(self, docs: DocumentArray, parameters: dict = {}, **kwargs):\n",
        "        \"\"\"\n",
        "        Compute embeddings and store them in the `docs` array.\n",
        "\n",
        "        :param docs: documents sent to the encoder. The docs must have `text`.\n",
        "            By default, the input `text` must be a `list` of `str`.\n",
        "        :param parameters: dictionary to define the `access_paths` and the\n",
        "            `batch_size`. For example, `parameters={'access_paths': ['r'],\n",
        "            'batch_size': 10}`.\n",
        "        :param kwargs: Additional key value arguments.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        document_batches_generator = DocumentArray(\n",
        "            docs[parameters.get('access_paths', self.access_paths)],\n",
        "        ).batch(batch_size=parameters.get('batch_size', self.batch_size))\n",
        "\n",
        "        for batch_docs in document_batches_generator:\n",
        "            tensor_shape_list, mel_list = self._get_input_feature(batch_docs)\n",
        "            try:\n",
        "                mel_array = np.vstack(mel_list)\n",
        "            except ValueError as e:\n",
        "                self.logger.error(f'the blob must have the same size, {e}')\n",
        "                continue\n",
        "            [embeddings] = self.sess.run(\n",
        "                [self.embedding_tensor],\n",
        "                feed_dict={self.feature_tensor: mel_array}\n",
        "            )\n",
        "            result = self.post_processor.postprocess(embeddings)\n",
        "            beg = 0\n",
        "            for doc, tensor_shape in zip(batch_docs, tensor_shape_list):\n",
        "                if tensor_shape == 0:\n",
        "                    continue\n",
        "                emb = result[beg:beg+tensor_shape, :]\n",
        "                beg += tensor_shape\n",
        "                doc.embedding = np.float32(emb[:self.min_duration]).flatten()\n",
        "\n",
        "    def _get_input_feature(self, batch_docs):\n",
        "        mel_list = []\n",
        "        tensor_shape_list = []\n",
        "        if self._input == 'uri':\n",
        "            for doc in batch_docs:\n",
        "                f_suffix = Path(doc.uri).suffix\n",
        "                if f_suffix == '.wav':\n",
        "                    tensor = wavfile_to_examples(doc.uri)\n",
        "                elif f_suffix == '.mp3':\n",
        "                    tensor = mp3file_to_examples(doc.uri)\n",
        "                else:\n",
        "                    self.logger.warning(f'unsupported format {f_suffix}. Please use .mp3 or .wav')\n",
        "                    self.logger.warning(f'skip {doc.uri}')\n",
        "                    tensor_shape_list.append(0)\n",
        "                    continue\n",
        "                if tensor.shape[0] < self.min_duration:\n",
        "                    tensor_shape_list.append(0)\n",
        "                    continue\n",
        "                mel_list.append(tensor)\n",
        "                tensor_shape_list.append(tensor.shape[0])\n",
        "        elif self._input == 'waveform':\n",
        "            for doc in batch_docs:\n",
        "                data = doc.tensor\n",
        "                sr = doc.tags['sample_rate']\n",
        "                if len(data.shape) > 1:\n",
        "                    data = np.mean(data, axis=0)\n",
        "                samples = data / 32768.0  # Convert to [-1.0, +1.0]\n",
        "                tensor = waveform_to_examples(samples, sr)\n",
        "                if tensor.shape[0] < self.min_duration:\n",
        "                    tensor_shape_list.append(0)\n",
        "                    continue\n",
        "                tensor_shape_list.append(tensor.shape[0])\n",
        "                mel_list.append(tensor)\n",
        "        elif self._input == 'log_mel':\n",
        "            _mel_list = batch_docs.tensors\n",
        "            for tensor in _mel_list:\n",
        "                if tensor.shape[0] < self.min_duration:\n",
        "                    tensor_shape_list.append(0)\n",
        "                    continue\n",
        "                else:\n",
        "                    tensor_shape_list.append(tensor.shape[0])\n",
        "                mel_list.append(tensor)\n",
        "        return tensor_shape_list, mel_list\n",
        "\n",
        "    def close(self):\n",
        "        self.sess.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5b27KnT1vdu"
      },
      "source": [
        "# ~~Audio Adaptation Network (Train)~~\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joOVIPLU12hz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZKyDoozDG_O"
      },
      "source": [
        "# Pixel Decoder (Mask2former, Frozen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDS4NAauFcHI"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# fpn.py\n",
        "import logging\n",
        "import numpy as np\n",
        "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import fvcore.nn.weight_init as weight_init\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.init import xavier_uniform_, constant_, uniform_, normal_\n",
        "from torch.cuda.amp import autocast\n",
        "\n",
        "from detectron2.config import configurable\n",
        "from detectron2.layers import Conv2d, DeformConv, ShapeSpec, get_norm\n",
        "from detectron2.modeling import SEM_SEG_HEADS_REGISTRY\n",
        "\n",
        "from ..transformer_decoder.position_encoding import PositionEmbeddingSine\n",
        "from ..transformer_decoder.transformer import TransformerEncoder, TransformerEncoderLayer, _get_clones, _get_activation_fn\n",
        "\n",
        "\n",
        "def build_pixel_decoder(cfg, input_shape):\n",
        "    \"\"\"\n",
        "    Build a pixel decoder from `cfg.MODEL.MASK_FORMER.PIXEL_DECODER_NAME`.\n",
        "    \"\"\"\n",
        "    name = cfg.MODEL.SEM_SEG_HEAD.PIXEL_DECODER_NAME\n",
        "    model = SEM_SEG_HEADS_REGISTRY.get(name)(cfg, input_shape)\n",
        "    forward_features = getattr(model, \"forward_features\", None)\n",
        "    if not callable(forward_features):\n",
        "        raise ValueError(\n",
        "            \"Only SEM_SEG_HEADS with forward_features method can be used as pixel decoder. \"\n",
        "            f\"Please implement forward_features for {name} to only return mask features.\"\n",
        "        )\n",
        "    return model\n",
        "\n",
        "\n",
        "# This is a modified FPN decoder.\n",
        "@SEM_SEG_HEADS_REGISTRY.register()\n",
        "class BasePixelDecoder(nn.Module):\n",
        "    @configurable\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_shape: Dict[str, ShapeSpec],\n",
        "        *,\n",
        "        conv_dim: int,\n",
        "        mask_dim: int,\n",
        "        norm: Optional[Union[str, Callable]] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        NOTE: this interface is experimental.\n",
        "        Args:\n",
        "            input_shape: shapes (channels and stride) of the input features\n",
        "            conv_dims: number of output channels for the intermediate conv layers.\n",
        "            mask_dim: number of output channels for the final conv layer.\n",
        "            norm (str or callable): normalization for all conv layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        input_shape = sorted(input_shape.items(), key=lambda x: x[1].stride)\n",
        "        self.in_features = [k for k, v in input_shape]  # starting from \"res2\" to \"res5\"\n",
        "        feature_channels = [v.channels for k, v in input_shape]\n",
        "\n",
        "        lateral_convs = []\n",
        "        output_convs = []\n",
        "\n",
        "        use_bias = norm == \"\"\n",
        "        for idx, in_channels in enumerate(feature_channels):\n",
        "            if idx == len(self.in_features) - 1:\n",
        "                output_norm = get_norm(norm, conv_dim)\n",
        "                output_conv = Conv2d(\n",
        "                    in_channels,\n",
        "                    conv_dim,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=1,\n",
        "                    bias=use_bias,\n",
        "                    norm=output_norm,\n",
        "                    activation=F.relu,\n",
        "                )\n",
        "                weight_init.c2_xavier_fill(output_conv)\n",
        "                self.add_module(\"layer_{}\".format(idx + 1), output_conv)\n",
        "\n",
        "                lateral_convs.append(None)\n",
        "                output_convs.append(output_conv)\n",
        "            else:\n",
        "                lateral_norm = get_norm(norm, conv_dim)\n",
        "                output_norm = get_norm(norm, conv_dim)\n",
        "\n",
        "                lateral_conv = Conv2d(\n",
        "                    in_channels, conv_dim, kernel_size=1, bias=use_bias, norm=lateral_norm\n",
        "                )\n",
        "                output_conv = Conv2d(\n",
        "                    conv_dim,\n",
        "                    conv_dim,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=1,\n",
        "                    bias=use_bias,\n",
        "                    norm=output_norm,\n",
        "                    activation=F.relu,\n",
        "                )\n",
        "                weight_init.c2_xavier_fill(lateral_conv)\n",
        "                weight_init.c2_xavier_fill(output_conv)\n",
        "                self.add_module(\"adapter_{}\".format(idx + 1), lateral_conv)\n",
        "                self.add_module(\"layer_{}\".format(idx + 1), output_conv)\n",
        "\n",
        "                lateral_convs.append(lateral_conv)\n",
        "                output_convs.append(output_conv)\n",
        "        # Place convs into top-down order (from low to high resolution)\n",
        "        # to make the top-down computation in forward clearer.\n",
        "        self.lateral_convs = lateral_convs[::-1]\n",
        "        self.output_convs = output_convs[::-1]\n",
        "\n",
        "        self.mask_dim = mask_dim\n",
        "        self.mask_features = Conv2d(\n",
        "            conv_dim,\n",
        "            mask_dim,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "        )\n",
        "        weight_init.c2_xavier_fill(self.mask_features)\n",
        "\n",
        "        self.maskformer_num_feature_levels = 3  # always use 3 scales\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, cfg, input_shape: Dict[str, ShapeSpec]):\n",
        "        ret = {}\n",
        "        ret[\"input_shape\"] = {\n",
        "            k: v for k, v in input_shape.items() if k in cfg.MODEL.SEM_SEG_HEAD.IN_FEATURES\n",
        "        }\n",
        "        ret[\"conv_dim\"] = cfg.MODEL.SEM_SEG_HEAD.CONVS_DIM\n",
        "        ret[\"mask_dim\"] = cfg.MODEL.SEM_SEG_HEAD.MASK_DIM\n",
        "        ret[\"norm\"] = cfg.MODEL.SEM_SEG_HEAD.NORM\n",
        "        return ret\n",
        "\n",
        "    def forward_features(self, features):\n",
        "        multi_scale_features = []\n",
        "        num_cur_levels = 0\n",
        "        # Reverse feature maps into top-down order (from low to high resolution)\n",
        "        for idx, f in enumerate(self.in_features[::-1]):\n",
        "            x = features[f]\n",
        "            lateral_conv = self.lateral_convs[idx]\n",
        "            output_conv = self.output_convs[idx]\n",
        "            if lateral_conv is None:\n",
        "                y = output_conv(x)\n",
        "            else:\n",
        "                cur_fpn = lateral_conv(x)\n",
        "                # Following FPN implementation, we use nearest upsampling here\n",
        "                y = cur_fpn + F.interpolate(y, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
        "                y = output_conv(y)\n",
        "            if num_cur_levels < self.maskformer_num_feature_levels:\n",
        "                multi_scale_features.append(y)\n",
        "                num_cur_levels += 1\n",
        "        return self.mask_features(y), None, multi_scale_features\n",
        "\n",
        "    def forward(self, features, targets=None):\n",
        "        logger = logging.getLogger(__name__)\n",
        "        logger.warning(\"Calling forward() may cause unpredicted behavior of PixelDecoder module.\")\n",
        "        return self.forward_features(features)\n",
        "\n",
        "\n",
        "class TransformerEncoderOnly(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model=512,\n",
        "        nhead=8,\n",
        "        num_encoder_layers=6,\n",
        "        dim_feedforward=2048,\n",
        "        dropout=0.1,\n",
        "        activation=\"relu\",\n",
        "        normalize_before=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        encoder_layer = TransformerEncoderLayer(\n",
        "            d_model, nhead, dim_feedforward, dropout, activation, normalize_before\n",
        "        )\n",
        "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
        "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, mask, pos_embed):\n",
        "        # flatten NxCxHxW to HWxNxC\n",
        "        bs, c, h, w = src.shape\n",
        "        src = src.flatten(2).permute(2, 0, 1)\n",
        "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
        "        if mask is not None:\n",
        "            mask = mask.flatten(1)\n",
        "\n",
        "        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n",
        "        return memory.permute(1, 2, 0).view(bs, c, h, w)\n",
        "\n",
        "\n",
        "# This is a modified FPN decoder with extra Transformer encoder that processes the lowest-resolution feature map.\n",
        "@SEM_SEG_HEADS_REGISTRY.register()\n",
        "class TransformerEncoderPixelDecoder(BasePixelDecoder):\n",
        "    @configurable\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_shape: Dict[str, ShapeSpec],\n",
        "        *,\n",
        "        transformer_dropout: float,\n",
        "        transformer_nheads: int,\n",
        "        transformer_dim_feedforward: int,\n",
        "        transformer_enc_layers: int,\n",
        "        transformer_pre_norm: bool,\n",
        "        conv_dim: int,\n",
        "        mask_dim: int,\n",
        "        norm: Optional[Union[str, Callable]] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        NOTE: this interface is experimental.\n",
        "        Args:\n",
        "            input_shape: shapes (channels and stride) of the input features\n",
        "            transformer_dropout: dropout probability in transformer\n",
        "            transformer_nheads: number of heads in transformer\n",
        "            transformer_dim_feedforward: dimension of feedforward network\n",
        "            transformer_enc_layers: number of transformer encoder layers\n",
        "            transformer_pre_norm: whether to use pre-layernorm or not\n",
        "            conv_dims: number of output channels for the intermediate conv layers.\n",
        "            mask_dim: number of output channels for the final conv layer.\n",
        "            norm (str or callable): normalization for all conv layers\n",
        "        \"\"\"\n",
        "        super().__init__(input_shape, conv_dim=conv_dim, mask_dim=mask_dim, norm=norm)\n",
        "\n",
        "        input_shape = sorted(input_shape.items(), key=lambda x: x[1].stride)\n",
        "        self.in_features = [k for k, v in input_shape]  # starting from \"res2\" to \"res5\"\n",
        "        feature_strides = [v.stride for k, v in input_shape]\n",
        "        feature_channels = [v.channels for k, v in input_shape]\n",
        "\n",
        "        in_channels = feature_channels[len(self.in_features) - 1]\n",
        "        self.input_proj = Conv2d(in_channels, conv_dim, kernel_size=1)\n",
        "        weight_init.c2_xavier_fill(self.input_proj)\n",
        "        self.transformer = TransformerEncoderOnly(\n",
        "            d_model=conv_dim,\n",
        "            dropout=transformer_dropout,\n",
        "            nhead=transformer_nheads,\n",
        "            dim_feedforward=transformer_dim_feedforward,\n",
        "            num_encoder_layers=transformer_enc_layers,\n",
        "            normalize_before=transformer_pre_norm,\n",
        "        )\n",
        "        N_steps = conv_dim // 2\n",
        "        self.pe_layer = PositionEmbeddingSine(N_steps, normalize=True)\n",
        "\n",
        "        # update layer\n",
        "        use_bias = norm == \"\"\n",
        "        output_norm = get_norm(norm, conv_dim)\n",
        "        output_conv = Conv2d(\n",
        "            conv_dim,\n",
        "            conv_dim,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "            bias=use_bias,\n",
        "            norm=output_norm,\n",
        "            activation=F.relu,\n",
        "        )\n",
        "        weight_init.c2_xavier_fill(output_conv)\n",
        "        delattr(self, \"layer_{}\".format(len(self.in_features)))\n",
        "        self.add_module(\"layer_{}\".format(len(self.in_features)), output_conv)\n",
        "        self.output_convs[0] = output_conv\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, cfg, input_shape: Dict[str, ShapeSpec]):\n",
        "        ret = super().from_config(cfg, input_shape)\n",
        "        ret[\"transformer_dropout\"] = cfg.MODEL.MASK_FORMER.DROPOUT\n",
        "        ret[\"transformer_nheads\"] = cfg.MODEL.MASK_FORMER.NHEADS\n",
        "        ret[\"transformer_dim_feedforward\"] = cfg.MODEL.MASK_FORMER.DIM_FEEDFORWARD\n",
        "        ret[\n",
        "            \"transformer_enc_layers\"\n",
        "        ] = cfg.MODEL.SEM_SEG_HEAD.TRANSFORMER_ENC_LAYERS  # a separate config\n",
        "        ret[\"transformer_pre_norm\"] = cfg.MODEL.MASK_FORMER.PRE_NORM\n",
        "        return ret\n",
        "\n",
        "    def forward_features(self, features):\n",
        "        multi_scale_features = []\n",
        "        num_cur_levels = 0\n",
        "        # Reverse feature maps into top-down order (from low to high resolution)\n",
        "        for idx, f in enumerate(self.in_features[::-1]):\n",
        "            x = features[f]\n",
        "            lateral_conv = self.lateral_convs[idx]\n",
        "            output_conv = self.output_convs[idx]\n",
        "            if lateral_conv is None:\n",
        "                transformer = self.input_proj(x)\n",
        "                pos = self.pe_layer(x)\n",
        "                transformer = self.transformer(transformer, None, pos)\n",
        "                y = output_conv(transformer)\n",
        "                # save intermediate feature as input to Transformer decoder\n",
        "                transformer_encoder_features = transformer\n",
        "            else:\n",
        "                cur_fpn = lateral_conv(x)\n",
        "                # Following FPN implementation, we use nearest upsampling here\n",
        "                y = cur_fpn + F.interpolate(y, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
        "                y = output_conv(y)\n",
        "            if num_cur_levels < self.maskformer_num_feature_levels:\n",
        "                multi_scale_features.append(y)\n",
        "                num_cur_levels += 1\n",
        "        return self.mask_features(y), transformer_encoder_features, multi_scale_features\n",
        "\n",
        "    def forward(self, features, targets=None):\n",
        "        logger = logging.getLogger(__name__)\n",
        "        logger.warning(\"Calling forward() may cause unpredicted behavior of PixelDecoder module.\")\n",
        "        return self.forward_features(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIe03y3cLonc"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# msdeformattn.py\n",
        "import logging\n",
        "import numpy as np\n",
        "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import fvcore.nn.weight_init as weight_init\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.init import xavier_uniform_, constant_, uniform_, normal_\n",
        "from torch.cuda.amp import autocast\n",
        "\n",
        "from detectron2.config import configurable\n",
        "from detectron2.layers import Conv2d, ShapeSpec, get_norm\n",
        "from detectron2.modeling import SEM_SEG_HEADS_REGISTRY\n",
        "\n",
        "from ..transformer_decoder.position_encoding import PositionEmbeddingSine\n",
        "from ..transformer_decoder.transformer import _get_clones, _get_activation_fn\n",
        "from .ops.modules import MSDeformAttn\n",
        "\n",
        "\n",
        "# MSDeformAttn Transformer encoder in deformable detr\n",
        "class MSDeformAttnTransformerEncoderOnly(nn.Module):\n",
        "    def __init__(self, d_model=256, nhead=8,\n",
        "                 num_encoder_layers=6, dim_feedforward=1024, dropout=0.1,\n",
        "                 activation=\"relu\",\n",
        "                 num_feature_levels=4, enc_n_points=4,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "        encoder_layer = MSDeformAttnTransformerEncoderLayer(d_model, dim_feedforward,\n",
        "                                                            dropout, activation,\n",
        "                                                            num_feature_levels, nhead, enc_n_points)\n",
        "        self.encoder = MSDeformAttnTransformerEncoder(encoder_layer, num_encoder_layers)\n",
        "\n",
        "        self.level_embed = nn.Parameter(torch.Tensor(num_feature_levels, d_model))\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, MSDeformAttn):\n",
        "                m._reset_parameters()\n",
        "        normal_(self.level_embed)\n",
        "\n",
        "    def get_valid_ratio(self, mask):\n",
        "        _, H, W = mask.shape\n",
        "        valid_H = torch.sum(~mask[:, :, 0], 1)\n",
        "        valid_W = torch.sum(~mask[:, 0, :], 1)\n",
        "        valid_ratio_h = valid_H.float() / H\n",
        "        valid_ratio_w = valid_W.float() / W\n",
        "        valid_ratio = torch.stack([valid_ratio_w, valid_ratio_h], -1)\n",
        "        return valid_ratio\n",
        "\n",
        "    def forward(self, srcs, pos_embeds):\n",
        "        masks = [torch.zeros((x.size(0), x.size(2), x.size(3)), device=x.device, dtype=torch.bool) for x in srcs]\n",
        "        # prepare input for encoder\n",
        "        src_flatten = []\n",
        "        mask_flatten = []\n",
        "        lvl_pos_embed_flatten = []\n",
        "        spatial_shapes = []\n",
        "        for lvl, (src, mask, pos_embed) in enumerate(zip(srcs, masks, pos_embeds)):\n",
        "            bs, c, h, w = src.shape\n",
        "            spatial_shape = (h, w)\n",
        "            spatial_shapes.append(spatial_shape)\n",
        "            src = src.flatten(2).transpose(1, 2)\n",
        "            mask = mask.flatten(1)\n",
        "            pos_embed = pos_embed.flatten(2).transpose(1, 2)\n",
        "            lvl_pos_embed = pos_embed + self.level_embed[lvl].view(1, 1, -1)\n",
        "            lvl_pos_embed_flatten.append(lvl_pos_embed)\n",
        "            src_flatten.append(src)\n",
        "            mask_flatten.append(mask)\n",
        "        src_flatten = torch.cat(src_flatten, 1)\n",
        "        mask_flatten = torch.cat(mask_flatten, 1)\n",
        "        lvl_pos_embed_flatten = torch.cat(lvl_pos_embed_flatten, 1)\n",
        "        spatial_shapes = torch.as_tensor(spatial_shapes, dtype=torch.long, device=src_flatten.device)\n",
        "        level_start_index = torch.cat((spatial_shapes.new_zeros((1, )), spatial_shapes.prod(1).cumsum(0)[:-1]))\n",
        "        valid_ratios = torch.stack([self.get_valid_ratio(m) for m in masks], 1)\n",
        "\n",
        "        # encoder\n",
        "        memory = self.encoder(src_flatten, spatial_shapes, level_start_index, valid_ratios, lvl_pos_embed_flatten, mask_flatten)\n",
        "\n",
        "        return memory, spatial_shapes, level_start_index\n",
        "\n",
        "\n",
        "class MSDeformAttnTransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model=256, d_ffn=1024,\n",
        "                 dropout=0.1, activation=\"relu\",\n",
        "                 n_levels=4, n_heads=8, n_points=4):\n",
        "        super().__init__()\n",
        "\n",
        "        # self attention\n",
        "        self.self_attn = MSDeformAttn(d_model, n_levels, n_heads, n_points)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # ffn\n",
        "        self.linear1 = nn.Linear(d_model, d_ffn)\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(d_ffn, d_model)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    @staticmethod\n",
        "    def with_pos_embed(tensor, pos):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_ffn(self, src):\n",
        "        src2 = self.linear2(self.dropout2(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout3(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "    def forward(self, src, pos, reference_points, spatial_shapes, level_start_index, padding_mask=None):\n",
        "        # self attention\n",
        "        src2 = self.self_attn(self.with_pos_embed(src, pos), reference_points, src, spatial_shapes, level_start_index, padding_mask)\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "\n",
        "        # ffn\n",
        "        src = self.forward_ffn(src)\n",
        "\n",
        "        return src\n",
        "\n",
        "\n",
        "class MSDeformAttnTransformerEncoder(nn.Module):\n",
        "    def __init__(self, encoder_layer, num_layers):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    @staticmethod\n",
        "    def get_reference_points(spatial_shapes, valid_ratios, device):\n",
        "        reference_points_list = []\n",
        "        for lvl, (H_, W_) in enumerate(spatial_shapes):\n",
        "\n",
        "            ref_y, ref_x = torch.meshgrid(torch.linspace(0.5, H_ - 0.5, H_, dtype=torch.float32, device=device),\n",
        "                                          torch.linspace(0.5, W_ - 0.5, W_, dtype=torch.float32, device=device))\n",
        "            ref_y = ref_y.reshape(-1)[None] / (valid_ratios[:, None, lvl, 1] * H_)\n",
        "            ref_x = ref_x.reshape(-1)[None] / (valid_ratios[:, None, lvl, 0] * W_)\n",
        "            ref = torch.stack((ref_x, ref_y), -1)\n",
        "            reference_points_list.append(ref)\n",
        "        reference_points = torch.cat(reference_points_list, 1)\n",
        "        reference_points = reference_points[:, :, None] * valid_ratios[:, None]\n",
        "        return reference_points\n",
        "\n",
        "    def forward(self, src, spatial_shapes, level_start_index, valid_ratios, pos=None, padding_mask=None):\n",
        "        output = src\n",
        "        reference_points = self.get_reference_points(spatial_shapes, valid_ratios, device=src.device)\n",
        "        for _, layer in enumerate(self.layers):\n",
        "            output = layer(output, pos, reference_points, spatial_shapes, level_start_index, padding_mask)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "@SEM_SEG_HEADS_REGISTRY.register()\n",
        "class MSDeformAttnPixelDecoder(nn.Module):\n",
        "    @configurable\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_shape: Dict[str, ShapeSpec],\n",
        "        *,\n",
        "        transformer_dropout: float,\n",
        "        transformer_nheads: int,\n",
        "        transformer_dim_feedforward: int,\n",
        "        transformer_enc_layers: int,\n",
        "        conv_dim: int,\n",
        "        mask_dim: int,\n",
        "        norm: Optional[Union[str, Callable]] = None,\n",
        "        # deformable transformer encoder args\n",
        "        transformer_in_features: List[str],\n",
        "        common_stride: int,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        NOTE: this interface is experimental.\n",
        "        Args:\n",
        "            input_shape: shapes (channels and stride) of the input features\n",
        "            transformer_dropout: dropout probability in transformer\n",
        "            transformer_nheads: number of heads in transformer\n",
        "            transformer_dim_feedforward: dimension of feedforward network\n",
        "            transformer_enc_layers: number of transformer encoder layers\n",
        "            conv_dims: number of output channels for the intermediate conv layers.\n",
        "            mask_dim: number of output channels for the final conv layer.\n",
        "            norm (str or callable): normalization for all conv layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        transformer_input_shape = {\n",
        "            k: v for k, v in input_shape.items() if k in transformer_in_features\n",
        "        }\n",
        "\n",
        "        # this is the input shape of pixel decoder\n",
        "        input_shape = sorted(input_shape.items(), key=lambda x: x[1].stride)\n",
        "        self.in_features = [k for k, v in input_shape]  # starting from \"res2\" to \"res5\"\n",
        "        self.feature_strides = [v.stride for k, v in input_shape]\n",
        "        self.feature_channels = [v.channels for k, v in input_shape]\n",
        "\n",
        "        # this is the input shape of transformer encoder (could use less features than pixel decoder\n",
        "        transformer_input_shape = sorted(transformer_input_shape.items(), key=lambda x: x[1].stride)\n",
        "        self.transformer_in_features = [k for k, v in transformer_input_shape]  # starting from \"res2\" to \"res5\"\n",
        "        transformer_in_channels = [v.channels for k, v in transformer_input_shape]\n",
        "        self.transformer_feature_strides = [v.stride for k, v in transformer_input_shape]  # to decide extra FPN layers\n",
        "\n",
        "        self.transformer_num_feature_levels = len(self.transformer_in_features)\n",
        "        if self.transformer_num_feature_levels > 1:\n",
        "            input_proj_list = []\n",
        "            # from low resolution to high resolution (res5 -> res2)\n",
        "            for in_channels in transformer_in_channels[::-1]:\n",
        "                input_proj_list.append(nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, conv_dim, kernel_size=1),\n",
        "                    nn.GroupNorm(32, conv_dim),\n",
        "                ))\n",
        "            self.input_proj = nn.ModuleList(input_proj_list)\n",
        "        else:\n",
        "            self.input_proj = nn.ModuleList([\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(transformer_in_channels[-1], conv_dim, kernel_size=1),\n",
        "                    nn.GroupNorm(32, conv_dim),\n",
        "                )])\n",
        "\n",
        "        for proj in self.input_proj:\n",
        "            nn.init.xavier_uniform_(proj[0].weight, gain=1)\n",
        "            nn.init.constant_(proj[0].bias, 0)\n",
        "\n",
        "        self.transformer = MSDeformAttnTransformerEncoderOnly(\n",
        "            d_model=conv_dim,\n",
        "            dropout=transformer_dropout,\n",
        "            nhead=transformer_nheads,\n",
        "            dim_feedforward=transformer_dim_feedforward,\n",
        "            num_encoder_layers=transformer_enc_layers,\n",
        "            num_feature_levels=self.transformer_num_feature_levels,\n",
        "        )\n",
        "        N_steps = conv_dim // 2\n",
        "        self.pe_layer = PositionEmbeddingSine(N_steps, normalize=True)\n",
        "\n",
        "        self.mask_dim = mask_dim\n",
        "        # use 1x1 conv instead\n",
        "        self.mask_features = Conv2d(\n",
        "            conv_dim,\n",
        "            mask_dim,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "        )\n",
        "        weight_init.c2_xavier_fill(self.mask_features)\n",
        "\n",
        "        self.maskformer_num_feature_levels = 3  # always use 3 scales\n",
        "        self.common_stride = common_stride\n",
        "\n",
        "        # extra fpn levels\n",
        "        stride = min(self.transformer_feature_strides)\n",
        "        self.num_fpn_levels = int(np.log2(stride) - np.log2(self.common_stride))\n",
        "\n",
        "        lateral_convs = []\n",
        "        output_convs = []\n",
        "\n",
        "        use_bias = norm == \"\"\n",
        "        for idx, in_channels in enumerate(self.feature_channels[:self.num_fpn_levels]):\n",
        "            lateral_norm = get_norm(norm, conv_dim)\n",
        "            output_norm = get_norm(norm, conv_dim)\n",
        "\n",
        "            lateral_conv = Conv2d(\n",
        "                in_channels, conv_dim, kernel_size=1, bias=use_bias, norm=lateral_norm\n",
        "            )\n",
        "            output_conv = Conv2d(\n",
        "                conv_dim,\n",
        "                conv_dim,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1,\n",
        "                bias=use_bias,\n",
        "                norm=output_norm,\n",
        "                activation=F.relu,\n",
        "            )\n",
        "            weight_init.c2_xavier_fill(lateral_conv)\n",
        "            weight_init.c2_xavier_fill(output_conv)\n",
        "            self.add_module(\"adapter_{}\".format(idx + 1), lateral_conv)\n",
        "            self.add_module(\"layer_{}\".format(idx + 1), output_conv)\n",
        "\n",
        "            lateral_convs.append(lateral_conv)\n",
        "            output_convs.append(output_conv)\n",
        "        # Place convs into top-down order (from low to high resolution)\n",
        "        # to make the top-down computation in forward clearer.\n",
        "        self.lateral_convs = lateral_convs[::-1]\n",
        "        self.output_convs = output_convs[::-1]\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, cfg, input_shape: Dict[str, ShapeSpec]):\n",
        "        ret = {}\n",
        "        ret[\"input_shape\"] = {\n",
        "            k: v for k, v in input_shape.items() if k in cfg.MODEL.SEM_SEG_HEAD.IN_FEATURES\n",
        "        }\n",
        "        ret[\"conv_dim\"] = cfg.MODEL.SEM_SEG_HEAD.CONVS_DIM\n",
        "        ret[\"mask_dim\"] = cfg.MODEL.SEM_SEG_HEAD.MASK_DIM\n",
        "        ret[\"norm\"] = cfg.MODEL.SEM_SEG_HEAD.NORM\n",
        "        ret[\"transformer_dropout\"] = cfg.MODEL.MASK_FORMER.DROPOUT\n",
        "        ret[\"transformer_nheads\"] = cfg.MODEL.MASK_FORMER.NHEADS\n",
        "        # ret[\"transformer_dim_feedforward\"] = cfg.MODEL.MASK_FORMER.DIM_FEEDFORWARD\n",
        "        ret[\"transformer_dim_feedforward\"] = 1024  # use 1024 for deformable transformer encoder\n",
        "        ret[\n",
        "            \"transformer_enc_layers\"\n",
        "        ] = cfg.MODEL.SEM_SEG_HEAD.TRANSFORMER_ENC_LAYERS  # a separate config\n",
        "        ret[\"transformer_in_features\"] = cfg.MODEL.SEM_SEG_HEAD.DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES\n",
        "        ret[\"common_stride\"] = cfg.MODEL.SEM_SEG_HEAD.COMMON_STRIDE\n",
        "        return ret\n",
        "\n",
        "    @autocast(enabled=False)\n",
        "    def forward_features(self, features):\n",
        "        srcs = []\n",
        "        pos = []\n",
        "        # Reverse feature maps into top-down order (from low to high resolution)\n",
        "        for idx, f in enumerate(self.transformer_in_features[::-1]):\n",
        "            x = features[f].float()  # deformable detr does not support half precision\n",
        "            srcs.append(self.input_proj[idx](x))\n",
        "            pos.append(self.pe_layer(x))\n",
        "\n",
        "        y, spatial_shapes, level_start_index = self.transformer(srcs, pos)\n",
        "        bs = y.shape[0]\n",
        "\n",
        "        split_size_or_sections = [None] * self.transformer_num_feature_levels\n",
        "        for i in range(self.transformer_num_feature_levels):\n",
        "            if i < self.transformer_num_feature_levels - 1:\n",
        "                split_size_or_sections[i] = level_start_index[i + 1] - level_start_index[i]\n",
        "            else:\n",
        "                split_size_or_sections[i] = y.shape[1] - level_start_index[i]\n",
        "        y = torch.split(y, split_size_or_sections, dim=1)\n",
        "\n",
        "        out = []\n",
        "        multi_scale_features = []\n",
        "        num_cur_levels = 0\n",
        "        for i, z in enumerate(y):\n",
        "            out.append(z.transpose(1, 2).view(bs, -1, spatial_shapes[i][0], spatial_shapes[i][1]))\n",
        "\n",
        "        # append `out` with extra FPN levels\n",
        "        # Reverse feature maps into top-down order (from low to high resolution)\n",
        "        for idx, f in enumerate(self.in_features[:self.num_fpn_levels][::-1]):\n",
        "            x = features[f].float()\n",
        "            lateral_conv = self.lateral_convs[idx]\n",
        "            output_conv = self.output_convs[idx]\n",
        "            cur_fpn = lateral_conv(x)\n",
        "            # Following FPN implementation, we use nearest upsampling here\n",
        "            y = cur_fpn + F.interpolate(out[-1], size=cur_fpn.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "            y = output_conv(y)\n",
        "            out.append(y)\n",
        "\n",
        "        for o in out:\n",
        "            if num_cur_levels < self.maskformer_num_feature_levels:\n",
        "                multi_scale_features.append(o)\n",
        "                num_cur_levels += 1\n",
        "\n",
        "        return self.mask_features(out[-1]), out[0], multi_scale_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2PKKVcvDVVv"
      },
      "source": [
        "# Transformer Decoder (Mask2former, Frozen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG2-RNUGLpGk"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# Modified by Bowen Cheng from: https://github.com/facebookresearch/detr/blob/master/models/detr.py\n",
        "# mask2former/modeling/transformer_decoder/mask2former_transformer_decoder.py\n",
        "import logging\n",
        "import fvcore.nn.weight_init as weight_init\n",
        "from typing import Optional\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from detectron2.config import configurable\n",
        "from detectron2.layers import Conv2d\n",
        "\n",
        "from .position_encoding import PositionEmbeddingSine\n",
        "from .maskformer_transformer_decoder import TRANSFORMER_DECODER_REGISTRY\n",
        "\n",
        "\n",
        "class SelfAttentionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, nhead, dropout=0.0,\n",
        "                 activation=\"relu\", normalize_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self, tgt,\n",
        "                     tgt_mask: Optional[Tensor] = None,\n",
        "                     tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                     query_pos: Optional[Tensor] = None):\n",
        "        q = k = self.with_pos_embed(tgt, query_pos)\n",
        "        tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout(tgt2)\n",
        "        tgt = self.norm(tgt)\n",
        "\n",
        "        return tgt\n",
        "\n",
        "    def forward_pre(self, tgt,\n",
        "                    tgt_mask: Optional[Tensor] = None,\n",
        "                    tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                    query_pos: Optional[Tensor] = None):\n",
        "        tgt2 = self.norm(tgt)\n",
        "        q = k = self.with_pos_embed(tgt2, query_pos)\n",
        "        tgt2 = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout(tgt2)\n",
        "\n",
        "        return tgt\n",
        "\n",
        "    def forward(self, tgt,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(tgt, tgt_mask,\n",
        "                                    tgt_key_padding_mask, query_pos)\n",
        "        return self.forward_post(tgt, tgt_mask,\n",
        "                                 tgt_key_padding_mask, query_pos)\n",
        "\n",
        "\n",
        "class CrossAttentionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, nhead, dropout=0.0,\n",
        "                 activation=\"relu\", normalize_before=False):\n",
        "        super().__init__()\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self, tgt, memory,\n",
        "                     memory_mask: Optional[Tensor] = None,\n",
        "                     memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None,\n",
        "                     query_pos: Optional[Tensor] = None):\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout(tgt2)\n",
        "        tgt = self.norm(tgt)\n",
        "\n",
        "        return tgt\n",
        "\n",
        "    def forward_pre(self, tgt, memory,\n",
        "                    memory_mask: Optional[Tensor] = None,\n",
        "                    memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None,\n",
        "                    query_pos: Optional[Tensor] = None):\n",
        "        tgt2 = self.norm(tgt)\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout(tgt2)\n",
        "\n",
        "        return tgt\n",
        "\n",
        "    def forward(self, tgt, memory,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(tgt, memory, memory_mask,\n",
        "                                    memory_key_padding_mask, pos, query_pos)\n",
        "        return self.forward_post(tgt, memory, memory_mask,\n",
        "                                 memory_key_padding_mask, pos, query_pos)\n",
        "\n",
        "\n",
        "class FFNLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dim_feedforward=2048, dropout=0.0,\n",
        "                 activation=\"relu\", normalize_before=False):\n",
        "        super().__init__()\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self, tgt):\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout(tgt2)\n",
        "        tgt = self.norm(tgt)\n",
        "        return tgt\n",
        "\n",
        "    def forward_pre(self, tgt):\n",
        "        tgt2 = self.norm(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
        "        tgt = tgt + self.dropout(tgt2)\n",
        "        return tgt\n",
        "\n",
        "    def forward(self, tgt):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(tgt)\n",
        "        return self.forward_post(tgt)\n",
        "\n",
        "\n",
        "def _get_activation_fn(activation):\n",
        "    \"\"\"Return an activation function given a string\"\"\"\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    if activation == \"gelu\":\n",
        "        return F.gelu\n",
        "    if activation == \"glu\":\n",
        "        return F.glu\n",
        "    raise RuntimeError(F\"activation should be relu/gelu, not {activation}.\")\n",
        "\n",
        "# MLP\n",
        "class MLP(nn.Module):\n",
        "    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        h = [hidden_dim] * (num_layers - 1)\n",
        "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "@TRANSFORMER_DECODER_REGISTRY.register()\n",
        "class MultiScaleMaskedTransformerDecoder(nn.Module):\n",
        "\n",
        "    _version = 2\n",
        "\n",
        "    def _load_from_state_dict(\n",
        "        self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n",
        "    ):\n",
        "        version = local_metadata.get(\"version\", None)\n",
        "        if version is None or version < 2:\n",
        "            # Do not warn if train from scratch\n",
        "            scratch = True\n",
        "            logger = logging.getLogger(__name__)\n",
        "            for k in list(state_dict.keys()):\n",
        "                newk = k\n",
        "                if \"static_query\" in k:\n",
        "                    newk = k.replace(\"static_query\", \"query_feat\")\n",
        "                if newk != k:\n",
        "                    state_dict[newk] = state_dict[k]\n",
        "                    del state_dict[k]\n",
        "                    scratch = False\n",
        "\n",
        "            if not scratch:\n",
        "                logger.warning(\n",
        "                    f\"Weight format of {self.__class__.__name__} have changed! \"\n",
        "                    \"Please upgrade your models. Applying automatic conversion now ...\"\n",
        "                )\n",
        "\n",
        "    @configurable\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        mask_classification=True,\n",
        "        *,\n",
        "        num_classes: int,\n",
        "        hidden_dim: int,\n",
        "        num_queries: int,\n",
        "        nheads: int,\n",
        "        dim_feedforward: int,\n",
        "        dec_layers: int,\n",
        "        pre_norm: bool,\n",
        "        mask_dim: int,\n",
        "        enforce_input_project: bool,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        NOTE: this interface is experimental.\n",
        "        Args:\n",
        "            in_channels: channels of the input features\n",
        "            mask_classification: whether to add mask classifier or not\n",
        "            num_classes: number of classes\n",
        "            hidden_dim: Transformer feature dimension\n",
        "            num_queries: number of queries\n",
        "            nheads: number of heads\n",
        "            dim_feedforward: feature dimension in feedforward network\n",
        "            enc_layers: number of Transformer encoder layers\n",
        "            dec_layers: number of Transformer decoder layers\n",
        "            pre_norm: whether to use pre-LayerNorm or not\n",
        "            mask_dim: mask feature dimension\n",
        "            enforce_input_project: add input project 1x1 conv even if input\n",
        "                channels and hidden dim is identical\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        assert mask_classification, \"Only support mask classification model\"\n",
        "        self.mask_classification = mask_classification\n",
        "\n",
        "        # positional encoding\n",
        "        N_steps = hidden_dim // 2\n",
        "        self.pe_layer = PositionEmbeddingSine(N_steps, normalize=True)\n",
        "\n",
        "        # define Transformer decoder here\n",
        "        self.num_heads = nheads\n",
        "        self.num_layers = dec_layers\n",
        "        self.transformer_self_attention_layers = nn.ModuleList()\n",
        "        self.transformer_cross_attention_layers = nn.ModuleList()\n",
        "        self.transformer_ffn_layers = nn.ModuleList()\n",
        "\n",
        "        for _ in range(self.num_layers):\n",
        "            self.transformer_self_attention_layers.append(\n",
        "                SelfAttentionLayer(\n",
        "                    d_model=hidden_dim,\n",
        "                    nhead=nheads,\n",
        "                    dropout=0.0,\n",
        "                    normalize_before=pre_norm,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            self.transformer_cross_attention_layers.append(\n",
        "                CrossAttentionLayer(\n",
        "                    d_model=hidden_dim,\n",
        "                    nhead=nheads,\n",
        "                    dropout=0.0,\n",
        "                    normalize_before=pre_norm,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            self.transformer_ffn_layers.append(\n",
        "                FFNLayer(\n",
        "                    d_model=hidden_dim,\n",
        "                    dim_feedforward=dim_feedforward,\n",
        "                    dropout=0.0,\n",
        "                    normalize_before=pre_norm,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.decoder_norm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        self.num_queries = num_queries\n",
        "        # learnable query features\n",
        "        self.query_feat = nn.Embedding(num_queries, hidden_dim)\n",
        "        # learnable query p.e.\n",
        "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
        "\n",
        "        # level embedding (we always use 3 scales)\n",
        "        self.num_feature_levels = 3\n",
        "        self.level_embed = nn.Embedding(self.num_feature_levels, hidden_dim)\n",
        "        self.input_proj = nn.ModuleList()\n",
        "        for _ in range(self.num_feature_levels):\n",
        "            if in_channels != hidden_dim or enforce_input_project:\n",
        "                self.input_proj.append(Conv2d(in_channels, hidden_dim, kernel_size=1))\n",
        "                weight_init.c2_xavier_fill(self.input_proj[-1])\n",
        "            else:\n",
        "                self.input_proj.append(nn.Sequential())\n",
        "\n",
        "        # output FFNs\n",
        "        if self.mask_classification:\n",
        "            self.class_embed = nn.Linear(hidden_dim, num_classes + 1)\n",
        "        self.mask_embed = MLP(hidden_dim, hidden_dim, mask_dim, 3)\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, cfg, in_channels, mask_classification):\n",
        "        ret = {}\n",
        "        ret[\"in_channels\"] = in_channels\n",
        "        ret[\"mask_classification\"] = mask_classification\n",
        "\n",
        "        ret[\"num_classes\"] = cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES\n",
        "        ret[\"hidden_dim\"] = cfg.MODEL.MASK_FORMER.HIDDEN_DIM\n",
        "        ret[\"num_queries\"] = cfg.MODEL.MASK_FORMER.NUM_OBJECT_QUERIES\n",
        "        # Transformer parameters:\n",
        "        ret[\"nheads\"] = cfg.MODEL.MASK_FORMER.NHEADS\n",
        "        ret[\"dim_feedforward\"] = cfg.MODEL.MASK_FORMER.DIM_FEEDFORWARD\n",
        "\n",
        "        # NOTE: because we add learnable query features which requires supervision,\n",
        "        # we add minus 1 to decoder layers to be consistent with our loss\n",
        "        # implementation: that is, number of auxiliary losses is always\n",
        "        # equal to number of decoder layers. With learnable query features, the number of\n",
        "        # auxiliary losses equals number of decoders plus 1.\n",
        "        assert cfg.MODEL.MASK_FORMER.DEC_LAYERS >= 1\n",
        "        ret[\"dec_layers\"] = cfg.MODEL.MASK_FORMER.DEC_LAYERS - 1\n",
        "        ret[\"pre_norm\"] = cfg.MODEL.MASK_FORMER.PRE_NORM\n",
        "        ret[\"enforce_input_project\"] = cfg.MODEL.MASK_FORMER.ENFORCE_INPUT_PROJ\n",
        "\n",
        "        ret[\"mask_dim\"] = cfg.MODEL.SEM_SEG_HEAD.MASK_DIM\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def forward(self, x, mask_features, mask = None):\n",
        "        # x is a list of multi-scale feature\n",
        "        assert len(x) == self.num_feature_levels\n",
        "        src = []\n",
        "        pos = []\n",
        "        size_list = []\n",
        "\n",
        "        # disable mask, it does not affect performance\n",
        "        del mask\n",
        "\n",
        "        for i in range(self.num_feature_levels):\n",
        "            size_list.append(x[i].shape[-2:])\n",
        "            pos.append(self.pe_layer(x[i], None).flatten(2))\n",
        "            src.append(self.input_proj[i](x[i]).flatten(2) + self.level_embed.weight[i][None, :, None])\n",
        "\n",
        "            # flatten NxCxHxW to HWxNxC\n",
        "            pos[-1] = pos[-1].permute(2, 0, 1)\n",
        "            src[-1] = src[-1].permute(2, 0, 1)\n",
        "\n",
        "        _, bs, _ = src[0].shape\n",
        "\n",
        "        # QxNxC\n",
        "        query_embed = self.query_embed.weight.unsqueeze(1).repeat(1, bs, 1)\n",
        "        output = self.query_feat.weight.unsqueeze(1).repeat(1, bs, 1)\n",
        "\n",
        "        predictions_class = []\n",
        "        predictions_mask = []\n",
        "\n",
        "        # prediction heads on learnable query features\n",
        "        outputs_class, outputs_mask, attn_mask = self.forward_prediction_heads(output, mask_features, attn_mask_target_size=size_list[0])\n",
        "        predictions_class.append(outputs_class)\n",
        "        predictions_mask.append(outputs_mask)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            level_index = i % self.num_feature_levels\n",
        "            attn_mask[torch.where(attn_mask.sum(-1) == attn_mask.shape[-1])] = False\n",
        "            # attention: cross-attention first\n",
        "            output = self.transformer_cross_attention_layers[i](\n",
        "                output, src[level_index],\n",
        "                memory_mask=attn_mask,\n",
        "                memory_key_padding_mask=None,  # here we do not apply masking on padded region\n",
        "                pos=pos[level_index], query_pos=query_embed\n",
        "            )\n",
        "\n",
        "            output = self.transformer_self_attention_layers[i](\n",
        "                output, tgt_mask=None,\n",
        "                tgt_key_padding_mask=None,\n",
        "                query_pos=query_embed\n",
        "            )\n",
        "\n",
        "            # FFN\n",
        "            output = self.transformer_ffn_layers[i](\n",
        "                output\n",
        "            )\n",
        "\n",
        "            outputs_class, outputs_mask, attn_mask = self.forward_prediction_heads(output, mask_features, attn_mask_target_size=size_list[(i + 1) % self.num_feature_levels])\n",
        "            predictions_class.append(outputs_class)\n",
        "            predictions_mask.append(outputs_mask)\n",
        "\n",
        "        assert len(predictions_class) == self.num_layers + 1\n",
        "\n",
        "        out = {\n",
        "            'pred_logits': predictions_class[-1],\n",
        "            'pred_masks': predictions_mask[-1],\n",
        "            'aux_outputs': self._set_aux_loss(\n",
        "                predictions_class if self.mask_classification else None, predictions_mask\n",
        "            )\n",
        "        }\n",
        "        return out\n",
        "\n",
        "    def forward_prediction_heads(self, output, mask_features, attn_mask_target_size):\n",
        "        decoder_output = self.decoder_norm(output)\n",
        "        decoder_output = decoder_output.transpose(0, 1)\n",
        "        outputs_class = self.class_embed(decoder_output)\n",
        "        mask_embed = self.mask_embed(decoder_output)\n",
        "        outputs_mask = torch.einsum(\"bqc,bchw->bqhw\", mask_embed, mask_features)\n",
        "\n",
        "        # NOTE: prediction is of higher-resolution\n",
        "        # [B, Q, H, W] -> [B, Q, H*W] -> [B, h, Q, H*W] -> [B*h, Q, HW]\n",
        "        attn_mask = F.interpolate(outputs_mask, size=attn_mask_target_size, mode=\"bilinear\", align_corners=False)\n",
        "        # must use bool type\n",
        "        # If a BoolTensor is provided, positions with ``True`` are not allowed to attend while ``False`` values will be unchanged.\n",
        "        attn_mask = (attn_mask.sigmoid().flatten(2).unsqueeze(1).repeat(1, self.num_heads, 1, 1).flatten(0, 1) < 0.5).bool()\n",
        "        attn_mask = attn_mask.detach()\n",
        "\n",
        "        return outputs_class, outputs_mask, attn_mask\n",
        "\n",
        "    @torch.jit.unused\n",
        "    def _set_aux_loss(self, outputs_class, outputs_seg_masks):\n",
        "        # this is a workaround to make torchscript happy, as torchscript\n",
        "        # doesn't support dictionary with non-homogeneous values, such\n",
        "        # as a dict having both a Tensor and a list.\n",
        "        if self.mask_classification:\n",
        "            return [\n",
        "                {\"pred_logits\": a, \"pred_masks\": b}\n",
        "                for a, b in zip(outputs_class[:-1], outputs_seg_masks[:-1])\n",
        "            ]\n",
        "        else:\n",
        "            return [{\"pred_masks\": b} for b in outputs_seg_masks[:-1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "im15UM4AMo-A"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "MLP input: learned object queries -> object-conditioned audio feature\n",
        "\"\"\"\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        h = [hidden_dim] * (num_layers - 1)\n",
        "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
        "\n",
        "    def forward(self, x): # x is object-conditioned audio feature\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-z2HBpwPHMf"
      },
      "outputs": [],
      "source": [
        "class MultiScaleMaskedTransformerDecoder(nn.Module):\n",
        "\n",
        "    _version = 2\n",
        "\n",
        "    def _load_from_state_dict(\n",
        "        self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n",
        "    ):\n",
        "        version = local_metadata.get(\"version\", None)\n",
        "        if version is None or version < 2:\n",
        "            # Do not warn if train from scratch\n",
        "            scratch = True\n",
        "            logger = logging.getLogger(__name__)\n",
        "            for k in list(state_dict.keys()):\n",
        "                newk = k\n",
        "                if \"static_query\" in k:\n",
        "                    newk = k.replace(\"static_query\", \"query_feat\")\n",
        "                if newk != k:\n",
        "                    state_dict[newk] = state_dict[k]\n",
        "                    del state_dict[k]\n",
        "                    scratch = False\n",
        "\n",
        "            if not scratch:\n",
        "                logger.warning(\n",
        "                    f\"Weight format of {self.__class__.__name__} have changed! \"\n",
        "                    \"Please upgrade your models. Applying automatic conversion now ...\"\n",
        "                )\n",
        "\n",
        "    @configurable\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        mask_classification=True,\n",
        "        *,\n",
        "        num_classes: int,\n",
        "        hidden_dim: int,\n",
        "        num_queries: int,\n",
        "        nheads: int,\n",
        "        dim_feedforward: int,\n",
        "        dec_layers: int,\n",
        "        pre_norm: bool,\n",
        "        mask_dim: int,\n",
        "        enforce_input_project: bool,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        NOTE: this interface is experimental.\n",
        "        Args:\n",
        "            in_channels: channels of the input features\n",
        "            mask_classification: whether to add mask classifier or not\n",
        "            num_classes: number of classes\n",
        "            hidden_dim: Transformer feature dimension\n",
        "            num_queries: number of queries\n",
        "            nheads: number of heads\n",
        "            dim_feedforward: feature dimension in feedforward network\n",
        "            enc_layers: number of Transformer encoder layers\n",
        "            dec_layers: number of Transformer decoder layers\n",
        "            pre_norm: whether to use pre-LayerNorm or not\n",
        "            mask_dim: mask feature dimension\n",
        "            enforce_input_project: add input project 1x1 conv even if input\n",
        "                channels and hidden dim is identical\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        assert mask_classification, \"Only support mask classification model\"\n",
        "        self.mask_classification = mask_classification\n",
        "\n",
        "        # positional encoding\n",
        "        N_steps = hidden_dim // 2\n",
        "        self.pe_layer = PositionEmbeddingSine(N_steps, normalize=True)\n",
        "\n",
        "        # define Transformer decoder here\n",
        "        self.num_heads = nheads\n",
        "        self.num_layers = dec_layers\n",
        "        self.transformer_self_attention_layers = nn.ModuleList()\n",
        "        self.transformer_cross_attention_layers = nn.ModuleList()\n",
        "        self.transformer_ffn_layers = nn.ModuleList()\n",
        "\n",
        "        for _ in range(self.num_layers):\n",
        "            self.transformer_self_attention_layers.append(\n",
        "                SelfAttentionLayer(\n",
        "                    d_model=hidden_dim,\n",
        "                    nhead=nheads,\n",
        "                    dropout=0.0,\n",
        "                    normalize_before=pre_norm,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            self.transformer_cross_attention_layers.append(\n",
        "                CrossAttentionLayer(\n",
        "                    d_model=hidden_dim,\n",
        "                    nhead=nheads,\n",
        "                    dropout=0.0,\n",
        "                    normalize_before=pre_norm,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            self.transformer_ffn_layers.append(\n",
        "                FFNLayer(\n",
        "                    d_model=hidden_dim,\n",
        "                    dim_feedforward=dim_feedforward,\n",
        "                    dropout=0.0,\n",
        "                    normalize_before=pre_norm,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.decoder_norm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        self.num_queries = num_queries\n",
        "        # learnable query features\n",
        "        self.query_feat = nn.Embedding(num_queries, hidden_dim)\n",
        "        # learnable query p.e.\n",
        "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
        "\n",
        "        # level embedding (we always use 3 scales)\n",
        "        self.num_feature_levels = 3\n",
        "        self.level_embed = nn.Embedding(self.num_feature_levels, hidden_dim)\n",
        "        self.input_proj = nn.ModuleList()\n",
        "        for _ in range(self.num_feature_levels):\n",
        "            if in_channels != hidden_dim or enforce_input_project:\n",
        "                self.input_proj.append(Conv2d(in_channels, hidden_dim, kernel_size=1))\n",
        "                weight_init.c2_xavier_fill(self.input_proj[-1])\n",
        "            else:\n",
        "                self.input_proj.append(nn.Sequential())\n",
        "\n",
        "        # output FFNs\n",
        "        if self.mask_classification:\n",
        "            self.class_embed = nn.Linear(hidden_dim, num_classes + 1)\n",
        "        self.mask_embed = MLP(hidden_dim, hidden_dim, mask_dim, 3)\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, cfg, in_channels, mask_classification):\n",
        "        ret = {}\n",
        "        ret[\"in_channels\"] = in_channels\n",
        "        #ret[\"mask_classification\"] = mask_classification # classification 정보 없음\n",
        "\n",
        "        # ret[\"num_classes\"] = cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES # class 정보 없음\n",
        "        ret[\"hidden_dim\"] = cfg.MODEL.MASK_FORMER.HIDDEN_DIM\n",
        "        ret[\"num_queries\"] = cfg.MODEL.MASK_FORMER.NUM_OBJECT_QUERIES # N\n",
        "        # Transformer parameters:\n",
        "        ret[\"nheads\"] = cfg.MODEL.MASK_FORMER.NHEADS\n",
        "        ret[\"dim_feedforward\"] = cfg.MODEL.MASK_FORMER.DIM_FEEDFORWARD\n",
        "\n",
        "        # NOTE: because we add learnable query features which requires supervision,\n",
        "        # we add minus 1 to decoder layers to be consistent with our loss\n",
        "        # implementation: that is, number of auxiliary losses is always\n",
        "        # equal to number of decoder layers. With learnable query features, the number of\n",
        "        # auxiliary losses equals number of decoders plus 1.\n",
        "        assert cfg.MODEL.MASK_FORMER.DEC_LAYERS >= 1\n",
        "        ret[\"dec_layers\"] = cfg.MODEL.MASK_FORMER.DEC_LAYERS - 1\n",
        "        ret[\"pre_norm\"] = cfg.MODEL.MASK_FORMER.PRE_NORM\n",
        "        ret[\"enforce_input_project\"] = cfg.MODEL.MASK_FORMER.ENFORCE_INPUT_PROJ\n",
        "\n",
        "        ret[\"mask_dim\"] = cfg.MODEL.SEM_SEG_HEAD.MASK_DIM\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def forward(self, x, mask_features, mask = None):\n",
        "        # x is a list of multi-scale feature\n",
        "        assert len(x) == self.num_feature_levels\n",
        "        src = []\n",
        "        pos = []\n",
        "        size_list = []\n",
        "\n",
        "        # disable mask, it does not affect performance\n",
        "        del mask\n",
        "\n",
        "        for i in range(self.num_feature_levels):\n",
        "            size_list.append(x[i].shape[-2:])\n",
        "            pos.append(self.pe_layer(x[i], None).flatten(2))\n",
        "            src.append(self.input_proj[i](x[i]).flatten(2) + self.level_embed.weight[i][None, :, None])\n",
        "\n",
        "            # flatten NxCxHxW to HWxNxC\n",
        "            pos[-1] = pos[-1].permute(2, 0, 1)\n",
        "            src[-1] = src[-1].permute(2, 0, 1)\n",
        "\n",
        "        _, bs, _ = src[0].shape\n",
        "\n",
        "        # QxNxC\n",
        "        query_embed = self.query_embed.weight.unsqueeze(1).repeat(1, bs, 1)\n",
        "        output = self.query_feat.weight.unsqueeze(1).repeat(1, bs, 1)\n",
        "\n",
        "        # predictions_class = []\n",
        "        predictions_mask = []\n",
        "\n",
        "        # trasnformer decoder output -> audio module -> object_conditioned_audio_feature -> mask prediction\n",
        "        decoder_output = self.forward_transformer_decoder(output, mask_features)\n",
        "        object_conditioned_audio_feature = self.audio_module()\n",
        "        # prediction heads on learnable query features\n",
        "        outputs_mask, attn_mask = self.forward_prediction_heads(object_conditioned_audio_feature, mask_features, attn_mask_target_size=size_list[0]) # output_class 없음\n",
        "        # predictions_class.append(outputs_class)\n",
        "        predictions_mask.append(outputs_mask)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            level_index = i % self.num_feature_levels\n",
        "            attn_mask[torch.where(attn_mask.sum(-1) == attn_mask.shape[-1])] = False\n",
        "            # attention: cross-attention first\n",
        "            output = self.transformer_cross_attention_layers[i](\n",
        "                output, src[level_index],\n",
        "                memory_mask=attn_mask,\n",
        "                memory_key_padding_mask=None,  # here we do not apply masking on padded region\n",
        "                pos=pos[level_index], query_pos=query_embed\n",
        "            )\n",
        "\n",
        "            output = self.transformer_self_attention_layers[i](\n",
        "                output, tgt_mask=None,\n",
        "                tgt_key_padding_mask=None,\n",
        "                query_pos=query_embed\n",
        "            )\n",
        "\n",
        "            # FFN\n",
        "            output = self.transformer_ffn_layers[i](\n",
        "                output\n",
        "            )\n",
        "\n",
        "            # trasnformer decoder output -> audio module -> object_conditioned_audio_feature -> mask prediction\n",
        "            decoder_output = self.forward_transformer_decoder(output, mask_features)\n",
        "            object_conditioned_audio_feature = self.audio_module()\n",
        "            outputs_mask, attn_mask = self.forward_prediction_heads(output, object_conditioned_audio_feature, attn_mask_target_size=size_list[(i + 1) % self.num_feature_levels])\n",
        "            # predictions_class.append(outputs_class)\n",
        "            predictions_mask.append(outputs_mask)\n",
        "\n",
        "        assert len(predictions_class) == self.num_layers + 1\n",
        "\n",
        "        out = {\n",
        "            # 'pred_logits': predictions_class[-1],\n",
        "            'pred_masks': predictions_mask[-1],\n",
        "            'aux_outputs': self._set_aux_loss(predictions_mask)\n",
        "            #(predictions_class if self.mask_classification else None, predictions_mask)\n",
        "        }\n",
        "        return out\n",
        "\n",
        "    # forward_prediction_heads -> input, output형태가 달라지므로 수정 필요\n",
        "    def forward_transformer_decoder(self, output, mask_features):\n",
        "        decoder_output = self.decoder_norm(output)\n",
        "        decoder_output = decoder_output.transpose(0, 1)\n",
        "        # outputs_class = self.class_embed(decoder_output)  # class 없음\n",
        "        return decoder_output # decoder output이 audio separation model에 contioning embed로\n",
        "\n",
        "    # audio module 추가 필요: separation + audio encoder + filtering\n",
        "    def audio_module():\n",
        "        return object_conditioned_audio_feature\n",
        "\n",
        "    def forward_prediction_heads(self, object_conditioned_audio_feature, mask_features, attn_mask_target_size):\n",
        "        # mask_embed = self.mask_embed(decoder_output) # MLP layer에 decoder output이 바로 들어가지 않음\n",
        "        mask_embed = self.mask_embed(object_conditioned_audio_feature) # MLP layer에 object-conditioned audio feature가 input\n",
        "        outputs_mask = torch.einsum(\"bqc,bchw->bqhw\", mask_embed, mask_features)\n",
        "\n",
        "        # NOTE: prediction is of higher-resolution\n",
        "        # [B, Q, H, W] -> [B, Q, H*W] -> [B, h, Q, H*W] -> [B*h, Q, HW]\n",
        "        attn_mask = F.interpolate(outputs_mask, size=attn_mask_target_size, mode=\"bilinear\", align_corners=False)\n",
        "        # must use bool type\n",
        "        # If a BoolTensor is provided, positions with ``True`` are not allowed to attend while ``False`` values will be unchanged.\n",
        "        attn_mask = (attn_mask.sigmoid().flatten(2).unsqueeze(1).repeat(1, self.num_heads, 1, 1).flatten(0, 1) < 0.5).bool()\n",
        "        attn_mask = attn_mask.detach()\n",
        "\n",
        "        return outputs_mask, attn_mask # outputs_class 필요 없음\n",
        "\n",
        "    @torch.jit.unused\n",
        "    def _set_aux_loss(self, outputs_seg_masks):\n",
        "        # this is a workaround to make torchscript happy, as torchscript\n",
        "        # doesn't support dictionary with non-homogeneous values, such\n",
        "        # as a dict having both a Tensor and a list.\n",
        "\n",
        "        # if self.mask_classification:\n",
        "        #     return [\n",
        "        #         {\"pred_logits\": a, \"pred_masks\": b}\n",
        "        #         for a, b in zip(outputs_class[:-1], outputs_seg_masks[:-1])\n",
        "        #     ]\n",
        "        # else:\n",
        "        return [{\"pred_masks\": b} for b in outputs_seg_masks[:-1]] # class가 없으므로 mask로만 loss 계산"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk7sGpx3Ddyl"
      },
      "source": [
        "# Mask Prediction(Segmentation) Network (Mask2former, Train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diHMJdDgLpnq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elPCgIhLEMPK"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcjWHO1UMW0A"
      },
      "source": [
        "1. Audio Separation Sum Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FUMLVGIiJ8P"
      },
      "source": [
        "$\\sum^N A_{sep} = A_{ori}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTUXG7qE3y9_"
      },
      "source": [
        "~~ 2. Audio Encoder가 들었을 법한 소리를 만들도록 <- Encoder가 Frozen ~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a76Qw9yC36EZ"
      },
      "source": [
        "3. N object 중 쓸모없는 소리?는 0에 수렴하도록 하는 Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C1bYrZN45HF"
      },
      "source": [
        "4. 쓸모없는 query/embedding에 해당하는 계산식이 loss에 영향을 주는 것을 줄이는 loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcoe10QVnLXu"
      },
      "source": [
        "$\\sum^N_{i=1} (1 - spectral\\_flatness(i)) \\cdot L_{another}(i)$\n",
        "\n",
        "[spectral_flatness](https://librosa.org/doc/latest/generated/librosa.feature.spectral_flatness.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55osYADb6GPo"
      },
      "source": [
        "5. Contrastive Loss\n",
        "  - 2-way contrastive\n",
        "  - audio n-features - visual n-map similarity score\n",
        "  - pair-wise measure\n",
        "  - 녹음본 중 39분 쯤\n",
        "  - overlap을 최소화\n",
        "    - 같은 영역의 score map이 다 높지 않게\n",
        "    - map끼리의 합을 minimize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUww8Ei282OE"
      },
      "source": [
        "6. Contrastive Loss 2\n",
        "  - Mixed Audio - Video 간의 Score (1~5의 합?)\n",
        "  - vs 다른 Video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZUubyXQ8uip"
      },
      "source": [
        "7. tf decoder output - audio feature comparison loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA2o5Hph7b7U"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_67dQ5dyS4ja"
      },
      "source": [
        "# Challenges"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L87o3PpQSoI9"
      },
      "source": [
        "Dataset : Music domain에서 모아서 test 해서 확장하는 방법?, audio crawling, 어느 정도의 noise는 어쩔 수가 없다-> scale을 늘리자, or 합주 영상을 crawling, 앙상블(or  band), ​\n",
        "\n",
        "Audioset datset 전체를 사용하는 방향으로 music만 사용할 경우 object bias 가 너무 클것​\n",
        "\n",
        "Training 단계에서 배운 것만 할 수 있다는 문제, motivation: scalability!!!​\n",
        "\n",
        "ASL(L2 distance) -> root 빼기​\n",
        "\n",
        "Spectral flatness -> 수학적으로 미분 가능한지 ​\n",
        "\n",
        "Power 가 아무리 커도 0 sf가 작거나 power가 크거나 problem 0.5 0.5면? 굴곡이 있는 신호면 1에 가깝게  그냥 둘을 곱하면 어떨까? 곱한게 딱 적당하지 않을까? Scale이 너무 작으면 log나 뭐 수행하는 것이 좋다.​\n",
        "\n",
        "종 모양을 어떻게 해야할까....​\n",
        "\n",
        "Ablation을 해야할 것 같다 . Audio feature를 건드리는 방향으로 새 MLP를 MLP앞에 추가하자! Temporal dimesion을 살릴 필요가 없다!!! T를 없애야한다!!!​\n",
        "\n",
        "Map을 single vector로 mean pooling(ablation)을 해서 하는 방향으로 해보자, 합치고 dot product를 하자!!!!, T를 죽이자​\n",
        "\n",
        "Mask embedding에서 하던 앞에서 하던 상관없다!! Ablation을 해보자!!!​\n",
        "\n",
        "Inference 단계에서 어떻게 할껀가? Sound를 하나로 뭉치는 유도장치가 필요할 것 같다!, noise token이 필요할 것 같다! Learned audio query를 하나 decoder 통과 안한 noise query를 만들자!!! Noise를 뭉치는 background sound를 모아놓는 localization을 방해하는 애들을 모으자​\n",
        "\n",
        "Score map loss: max 값으로 해서 ​\n",
        "\n",
        "(Inner contrastive loss 는 굳이 필요한가? Overlapping loss 만으로 충분하지 않을까? Ablation 필요!)​\n",
        "\n",
        "Outer contrastive loss 이건 잘못되었다. Score를 average해서 이걸 positive sample에 대한 N H W 로 다 더한걸 similarity로 생각하자 ​\n",
        "\n",
        "다른 오디오 와도 score map을 구해서 이걸 negative i를 포함한 모든 softmax로 해서 i를 maximize 즉 aggregation을 한다는 아이디어!!!​\n",
        "\n",
        "CLIP paper figure 1 참고!!!! ​\n",
        "\n",
        "노이즈를 모으는 token 을 만들면 좋겠다!!!!! N+1개의 signal을 만든다 -> loss를 similarity를 noise 에대한건 재지말고 similarity를 높이는 걸 방해하는 걸 거기에 넣는 audio encoder도 통과하지 않아도 되는 token!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-auIgZiZxUh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Prk6F156CUw6",
        "Kj6tTy25CYTp",
        "ddKURDH2OHQe",
        "fdz_MQwfDB7X",
        "o5b27KnT1vdu",
        "7ZKyDoozDG_O",
        "h2PKKVcvDVVv",
        "Wk7sGpx3Ddyl"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
