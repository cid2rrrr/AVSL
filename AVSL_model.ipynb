{"cells":[{"cell_type":"markdown","metadata":{"id":"p35qYqUBUAoj"},"source":["# Environment setup"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# ! python -m pip install detectron2 -f \\\n","#   https://dl.fbaipublicfiles.com/detectron2/wheels/cu113/torch1.10/index.html"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44,"status":"ok","timestamp":1706448529747,"user":{"displayName":"구은아","userId":"03420424854508517452"},"user_tz":-540},"id":"qvh1F6NuUAow","outputId":"3882b3b6-45b6-4493-c653-0093519c6b0b"},"outputs":[{"name":"stdout","output_type":"stream","text":["/workspace/GitHub/AVSL\n"]}],"source":["%cd /workspace/GitHub/AVSL"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import detectron2.utils.comm as comm\n","from detectron2.config import get_cfg\n","from detectron2.projects.deeplab import add_deeplab_config\n","from detectron2.utils.logger import setup_logger\n","\n","from MODULES.MaskFormer.config import add_mask_former_config"]},{"cell_type":"markdown","metadata":{},"source":["- init.py: from . import modeling\n","- modeling/init.py: from .pixel_decoder.msdeformattn import MSDeformAttnPixelDecoder\n","- modeling/pixel_decoder/msdeformattn.py: from .ops.modules import MSDeformAttn\n","- modeling/pixel_decoder/ops/modules/ms_deform_attn.py: from ..functions import MSDeformAttnFunction\n","- modeling/pixel_decoder/ops/functions/ms_deform_attn_func.py: import MultiScaleDeformableAttention as MSDA 주석 처리"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading config MODULES/MaskFormer/configs/custom/MaskAVSL_swin_base.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n"]},{"data":{"text/plain":["<Logger mask_former (DEBUG)>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Create configs and perform basic setups\n","\n","cfg = get_cfg()\n","add_deeplab_config(cfg)\n","add_mask_former_config(cfg)\n","cfg.set_new_allowed(True)\n","cfg.merge_from_file(\"MODULES/MaskFormer/configs/custom/MaskAVSL_swin_base.yaml\")\n","# cfg.merge_from_list(args.opts)\n","cfg.MODEL.DEVICE = \"cuda:1\"\n","cfg.SOLVER.IMS_PER_BATCH = 4\n","cfg.eval_only = True\n","cfg.freeze()\n","# default_setup(cfg, args)\n","# Setup logger for \"mask_former\" module\n","setup_logger(output=cfg.OUTPUT_DIR, distributed_rank=comm.get_rank(), name=\"mask_former\")"]},{"cell_type":"markdown","metadata":{},"source":["# Custom Dataset"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["34808 of videos have loaded\n"]}],"source":["from DATALOADER import VideoDataLoader\n","\n","folder_path = 'DATA/videos'\n","dataloader = VideoDataLoader(cfg, folder_path)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["8702\n"]},{"name":"stderr","output_type":"stream","text":["                                                                  \r"]}],"source":["print(len(dataloader))\n","\n","# detectron2/engine/train_loop.py\n","data = next(iter(dataloader))"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 4/4 [00:00<00:00, 53.92it/s]\n"]}],"source":["# maskformer_model.py\n","from detectron2.structures import ImageList\n","from tqdm import tqdm\n","import torch\n","\n","pixel_mean = cfg.MODEL.PIXEL_MEAN\n","pixel_std = cfg.MODEL.PIXEL_STD\n","size_divisibility = cfg.MODEL.MASK_FORMER.SIZE_DIVISIBILITY\n","device = cfg.MODEL.DEVICE\n","\n","pixel_mean = torch.Tensor(pixel_mean).view(-1, 1, 1)\n","pixel_std = torch.Tensor(pixel_std).view(-1, 1, 1)\n","\n","images = [x for x in data['image']]\n","images = [(x - pixel_mean) / pixel_std for x in tqdm(images)]\n","images = ImageList.from_tensors(images, size_divisibility)"]},{"cell_type":"markdown","metadata":{},"source":["### code"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import sys, os\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","from moviepy.editor import VideoFileClip\n","\n","from detectron2.config import configurable\n","from detectron2.data import transforms as T\n","from detectron2.projects.point_rend import ColorAugSSDTransform\n","from detectron2.data import detection_utils as utils"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class VideoDataset(Dataset):\n","    @configurable\n","    def __init__(self, is_train=True, *, augmentations, image_format, ignore_label, size_divisibility, folder_path, ):\n","        self.folder_path = folder_path\n","        self.video_list = [f for f in os.listdir(folder_path) if f.endswith('.mp4')]\n","        self.tfm_gens = augmentations\n","\n","        self.is_train = is_train\n","        self.img_format = image_format\n","        self.ignore_label = ignore_label\n","        self.size_divisibility = size_divisibility\n","\n","    def __len__(self):\n","        return len(self.video_list)\n","\n","    def __getitem__(self, idx):\n","        video_path = os.path.join(self.folder_path, self.video_list[idx])\n","\n","        # Load video and extract central frame\n","        video_clip = VideoFileClip(video_path)\n","        central_frame = video_clip.get_frame(video_clip.duration / 2)\n","        central_frame = np.array(central_frame)\n","        # Convert image frame to numpy array and normalize\n","        #central_frame = central_frame / 255.0\n","\n","        # Additional augmentation\n","        aug_input = T.AugInput(central_frame)\n","        aug_input, transforms = T.apply_transform_gens(self.tfm_gens, aug_input)\n","        central_frame = aug_input.image\n","        #sem_seg_gt = aug_input.sem_seg\n","\n","        central_frame = torch.as_tensor(np.ascontiguousarray(central_frame.transpose(2, 0, 1)))\n","        #central_frame = np.transpose(central_frame, (2, 0, 1))  # Change HWC to CHW\n","\n","        if self.size_divisibility > 0:\n","            central_frame_size = (central_frame.shape[-2], central_frame.shape[-1])\n","            padding_size = [\n","                0,\n","                self.size_divisibility - central_frame_size[1],\n","                0,\n","                self.size_divisibility - central_frame_size[0],\n","            ]\n","            central_frame = F.pad(central_frame, padding_size, value=128).contiguous()\n","\n","        sample = {}\n","        # sample['image'] = central_frame\n","\n","        # Save audio as WAV file\n","        audio_path = f\"{video_path[:-4]}.wav\"\n","        self.blockPrint()\n","        # audio = \n","        video_clip.audio.write_audiofile(audio_path, codec='pcm_s16le', fps=44100)\n","        self.enablePrint()\n","\n","        # Return data as dictionary\n","        sample = {'image': central_frame, 'audio_path': audio_path}\n","        # sample = {'image': central_frame, 'audio': audio}\n","\n","        return sample\n","\n","    @classmethod\n","    def from_config(cls, cfg, is_train=True):\n","        # Build augmentation\n","        augs = [\n","            T.ResizeShortestEdge(\n","                cfg.INPUT.MIN_SIZE_TRAIN,\n","                cfg.INPUT.MAX_SIZE_TRAIN,\n","                cfg.INPUT.MIN_SIZE_TRAIN_SAMPLING,\n","            )\n","        ]\n","        if cfg.INPUT.CROP.ENABLED:\n","            augs.append(\n","                T.RandomCrop_CategoryAreaConstraint(\n","                    cfg.INPUT.CROP.TYPE,\n","                    cfg.INPUT.CROP.SIZE,\n","                    cfg.INPUT.CROP.SINGLE_CATEGORY_MAX_AREA,\n","                    cfg.MODEL.SEM_SEG_HEAD.IGNORE_VALUE,\n","                )\n","            )\n","        if cfg.INPUT.COLOR_AUG_SSD:\n","            augs.append(ColorAugSSDTransform(img_format=cfg.INPUT.FORMAT))\n","        augs.append(T.RandomFlip())\n","        augs.extend([\n","        T.ResizeScale(\n","            min_scale=cfg.INPUT.MIN_SCALE, max_scale=cfg.INPUT.MAX_SCALE,\n","            target_height=cfg.INPUT.IMAGE_SIZE, target_width=cfg.INPUT.IMAGE_SIZE\n","        ),\n","        T.FixedSizeCrop(crop_size=(cfg.INPUT.IMAGE_SIZE, cfg.INPUT.IMAGE_SIZE)),\n","        ])\n","\n","        ignore_label = False\n","\n","        ret = {\n","            \"is_train\": is_train,\n","            \"augmentations\": augs,\n","            \"image_format\": cfg.INPUT.FORMAT,\n","            \"ignore_label\": ignore_label,\n","            \"size_divisibility\": cfg.INPUT.SIZE_DIVISIBILITY,\n","        }\n","        return ret\n","    \n","    def blockPrint(self):\n","        global backupstdout\n","        backupstdout=sys.stdout\n","        sys.stdout = open(os.devnull, 'w')\n","\n","    def enablePrint(self):\n","        global backupstdout\n","        sys.stdout = backupstdout"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["folder_path = 'DATA/videos'\n","video_dataset = VideoDataset(cfg, folder_path=folder_path)\n","dataloader = DataLoader(video_dataset, batch_size=cfg.SOLVER.IMS_PER_BATCH, shuffle=True)"]},{"cell_type":"markdown","metadata":{},"source":["### show"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# images = []\n","# for i, x in tqdm(enumerate(dataloader)): \n","#     img = x[\"image\"]\n","#     img = (img - pixel_mean) / pixel_std\n","#     images.append(img)\n","#     if i == 10:\n","#         break # 11개에 2분/이었는데 6분으로 늘어남 \n","# images = ImageList.from_tensors(images, size_divisibility)\n","# images.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# detectron2/modeling/meta_arch/build.py\n","# def build_model(cfg):\n","#     \"\"\"\n","#     Build the whole model architecture, defined by ``cfg.MODEL.META_ARCHITECTURE``.\n","#     Note that it does not load any weights from ``cfg``.\n","#     \"\"\"\n","#     meta_arch = cfg.MODEL.META_ARCHITECTURE\n","#     model = META_ARCH_REGISTRY.get(meta_arch)(cfg) # 'MaskFormer'\n","#     model.to(torch.device(cfg.MODEL.DEVICE))\n","#     _log_api_usage(\"modeling.meta_arch.\" + meta_arch)\n","#     return model\n","# -> model = maskformer_model.py\n","\n","# detectron2/engine/defaults.py\n","# data_loader = self.build_train_loader(cfg) -> torchdata.DataLoader(dataset, batch_size=batch_size,)\n","# _trainer = (AMPTrainer if cfg.SOLVER.AMP.ENABLED else SimpleTrainer)(model, data_loader, optimizer)\n","\n","# detectron2/engine/train_loop.py\n","# def _data_loader_iter(self):\n","#     # only create the data loader iterator when it is used\n","#     if self._data_loader_iter_obj is None:\n","#         self._data_loader_iter_obj = iter(self.data_loader)\n","#     return self._data_loader_iter_obj\n","# data = next(self._data_loader_iter) -> next(iter(self.data_loader)) \n","# model(data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# example = video_dataset.__getitem__(1)\n","# print(example.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# example['image'].shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# example['audio_path']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from matplotlib import pyplot as plt\n","# plt.imshow(example['image'].permute(1, 2, 0), interpolation='nearest')\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from scipy.io import wavfile\n","\n","# fs, audio_data = wavfile.read(example['audio_path'])\n","# print(fs, audio_data.shape)\n","\n","# # plt.figure(figsize = (12, 3))\n","# plt.plot(audio_data, lw = 1)\n","# plt.xlim(0, len(audio_data))"]},{"cell_type":"markdown","metadata":{"id":"Sm5D6upQUAoy"},"source":["# Model Architecture"]},{"cell_type":"markdown","metadata":{},"source":["## backbone"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/workspace/GitHub/AVSL\n"]}],"source":["%cd /workspace/GitHub/AVSL"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# model.py\n","from detectron2.layers import ShapeSpec\n","from MODULES.MaskFormer.modeling.backbone.swin import D2SwinTransformer\n","\n","def build_swin_backbone(cfg):\n","    input_shape = ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN))\n","    model = D2SwinTransformer(cfg, input_shape)\n","    model.init_weights(cfg.MODEL.WEIGHTS)\n","    return model\n","\n","backbone = build_swin_backbone(cfg)\n","# with torch.no_grad():\n","#     image_feature = backbone(images.tensor)\n","\n","# backbone.to(torch.device(device))\n","# with torch.no_grad():\n","#     image_feature = backbone(images.tensor.to(device))"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# from detectron2.modeling import build_backbone\n","# from detectron2.modeling import BACKBONE_REGISTRY\n","\n","# input_shape = ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN))\n","# backbone = build_backbone(cfg)\n","# backbone = BACKBONE_REGISTRY.get(\"D2SwinTransformer\")(cfg, input_shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from detectron2.layers import ShapeSpec\n","# from Mask2former.mask2former.modeling.backbone.swin import D2SwinTransformer\n","\n","# print(cfg.MODEL.WEIGHTS)\n","# input_shape = ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN))\n","# backbone = D2SwinTransformer(cfg, input_shape)\n","# backbone.init_weights(cfg.MODEL.WEIGHTS)\n","# # backbone.to(device)\n","\n","# features = backbone(images.tensor)"]},{"cell_type":"markdown","metadata":{},"source":["### load weights"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/workspace/GitHub/AVSL/MODULES/ckpt\n"]}],"source":["%cd MODULES/ckpt"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-02-19 08:44:27--  https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384_22k.pth\n","Resolving github.com (github.com)... 20.200.245.247\n","Connecting to github.com (github.com)|20.200.245.247|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/357198522/2a4d1980-9bd4-11eb-9482-36c4b4f6edc3?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240219%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240219T084427Z&X-Amz-Expires=300&X-Amz-Signature=c7c1f351946f4e404b971f046396bb8c5e918d91abc39382fcf8ac4761ab2a53&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=357198522&response-content-disposition=attachment%3B%20filename%3Dswin_base_patch4_window12_384_22k.pth&response-content-type=application%2Foctet-stream [following]\n","--2024-02-19 08:44:27--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/357198522/2a4d1980-9bd4-11eb-9482-36c4b4f6edc3?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240219%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240219T084427Z&X-Amz-Expires=300&X-Amz-Signature=c7c1f351946f4e404b971f046396bb8c5e918d91abc39382fcf8ac4761ab2a53&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=357198522&response-content-disposition=attachment%3B%20filename%3Dswin_base_patch4_window12_384_22k.pth&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 450809979 (430M) [application/octet-stream]\n","Saving to: ‘swin_base_patch4_window12_384_22k.pth’\n","\n","swin_base_patch4_wi 100%[===================>] 429.93M  1.97MB/s    in 1m 58s  \n","\n","2024-02-19 08:46:26 (3.64 MB/s) - ‘swin_base_patch4_window12_384_22k.pth’ saved [450809979/450809979]\n","\n"]}],"source":["!wget https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384_22k.pth"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["!python tool/convert-pretrained-swin-model-to-d2.py swin_base_patch4_window12_384_22k.pth swin_base_patch4_window12_384_22k.pkl"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/workspace/GitHub/AVSL\n"]}],"source":["%cd ../../"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["MODULES/ckpt/swin_base_patch4_window12_384_22k.pkl\n"]}],"source":["import os\n","weights_path = os.path.join(\"MODULES/ckpt\", cfg.MODEL.WEIGHTS)\n","print(weights_path)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["weights = torch.load(weights_path)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":["dict_keys(['model', '__author__', 'matching_heuristics'])"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["weights.keys()"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["weights['matching_heuristics']"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for D2SwinTransformer:\n\tMissing key(s) in state_dict: \"norm0.weight\", \"norm0.bias\", \"norm1.weight\", \"norm1.bias\", \"norm2.weight\", \"norm2.bias\", \"norm3.weight\", \"norm3.bias\". \n\tUnexpected key(s) in state_dict: \"norm.weight\", \"norm.bias\", \"head.weight\", \"head.bias\", \"layers.0.blocks.1.attn_mask\", \"layers.1.blocks.1.attn_mask\", \"layers.2.blocks.1.attn_mask\", \"layers.2.blocks.3.attn_mask\", \"layers.2.blocks.5.attn_mask\", \"layers.2.blocks.7.attn_mask\", \"layers.2.blocks.9.attn_mask\", \"layers.2.blocks.11.attn_mask\", \"layers.2.blocks.13.attn_mask\", \"layers.2.blocks.15.attn_mask\", \"layers.2.blocks.17.attn_mask\". ","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_3047328/2263083118.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/envs/GEA/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1483\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1484\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for D2SwinTransformer:\n\tMissing key(s) in state_dict: \"norm0.weight\", \"norm0.bias\", \"norm1.weight\", \"norm1.bias\", \"norm2.weight\", \"norm2.bias\", \"norm3.weight\", \"norm3.bias\". \n\tUnexpected key(s) in state_dict: \"norm.weight\", \"norm.bias\", \"head.weight\", \"head.bias\", \"layers.0.blocks.1.attn_mask\", \"layers.1.blocks.1.attn_mask\", \"layers.2.blocks.1.attn_mask\", \"layers.2.blocks.3.attn_mask\", \"layers.2.blocks.5.attn_mask\", \"layers.2.blocks.7.attn_mask\", \"layers.2.blocks.9.attn_mask\", \"layers.2.blocks.11.attn_mask\", \"layers.2.blocks.13.attn_mask\", \"layers.2.blocks.15.attn_mask\", \"layers.2.blocks.17.attn_mask\". "]}],"source":["backbone.load_state_dict(weights['model'])"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["from urllib.parse import parse_qs, urlparse\n","from detectron2.utils.file_io import PathManager\n","\n","path_manager = PathManager\n","\n","path = os.path.join(\"MODULES/ckpt\", cfg.MODEL.WEIGHTS)\n","parsed_url = urlparse(path)\n","_parsed_url_during_load = parsed_url\n","path = parsed_url._replace(query=\"\").geturl()\n","path = path_manager.get_local_path(path)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'D2SwinTransformer' object has no attribute 'logger'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_3116623/2620555749.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcheckpointer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCheckpointer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcheckpointer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/envs/GEA/lib/python3.7/site-packages/fvcore/common/checkpoint.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, checkpointables)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No checkpoint found. Initializing model from scratch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[Checkpointer] Loading from {} ...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/envs/GEA/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1178\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'D2SwinTransformer' object has no attribute 'logger'"]}],"source":["from fvcore.common.checkpoint import Checkpointer\n","\n","checkpointer = Checkpointer\n","checkpointer.load(backbone, path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loaded = weights"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["parsed_url = _parsed_url_during_load\n","queries = parse_qs(parsed_url.query)\n","\n","# if queries.pop(\"matching_heuristics\", \"False\") == [\"True\"]:\n","#     loaded[\"matching_heuristics\"] = True\n","# if len(queries) > 0:\n","#     raise ValueError(\n","#         f\"Unsupported query remaining: f{queries}, orginal filename: {parsed_url.geturl()}\"\n","#     )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["{}"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["queries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n"]}],"source":["import pickle\n","\n","with PathManager.open(path, \"rb\") as f:\n","    data = torch.load(f, encoding=\"latin1\")\n","if \"model\" in data and \"__author__\" in data:\n","    # file is in Detectron2 model zoo format\n","    self.logger.info(\"Reading a file from '{}'\".format(data[\"__author__\"]))"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"ename":"UnpicklingError","evalue":"A load persistent id instruction was encountered,\nbut no persistent_load function was specified.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_3116623/1839331306.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcheckpointer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDetectionCheckpointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcheckpointer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_or_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/envs/GEA/lib/python3.7/site-packages/fvcore/common/checkpoint.py\u001b[0m in \u001b[0;36mresume_or_load\u001b[0;34m(self, path, resume)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpointables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag_last_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_filename_basename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/envs/GEA/lib/python3.7/site-packages/detectron2/checkpoint/detection_checkpoint.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, *args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# don't load if not readable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mneed_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/envs/GEA/lib/python3.7/site-packages/fvcore/common/checkpoint.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, checkpointables)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Checkpoint {} not found!\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mincompatible\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         if (\n","\u001b[0;32m/opt/conda/envs/GEA/lib/python3.7/site-packages/detectron2/checkpoint/detection_checkpoint.py\u001b[0m in \u001b[0;36m_load_file\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mPathManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latin1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"model\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"__author__\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;31m# file is in Detectron2 model zoo format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnpicklingError\u001b[0m: A load persistent id instruction was encountered,\nbut no persistent_load function was specified."]}],"source":["from detectron2.checkpoint import DetectionCheckpointer\n","\n","checkpointer = DetectionCheckpointer(backbone, cfg.OUTPUT_DIR)\n","checkpointer.resume_or_load(path, resume=True)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["from fvcore.common.checkpoint import Checkpointer\n","\n","checkpoint = torch.load(weights_path)\n","checkpointer = Checkpointer(backbone)\n","checkpointer._convert_ndarray_to_tensor(checkpoint[\"model\"])"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["from detectron2.checkpoint.c2_model_loading import align_and_update_state_dicts\n","\n","checkpoint[\"model\"] = align_and_update_state_dicts(\n","                backbone.state_dict(),\n","                checkpoint[\"model\"],\n","                c2_conversion=False #checkpoint.get(\"__author__\", None) == \"Caffe2\",\n","            )"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":["False"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","backbone.load_state_dict(torch.load(\"swin_tiny_patch4_window7_224.pkl\"))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["dict_keys(['res2', 'res3', 'res4', 'res5'])"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["image_feature.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([16, 192, 256, 256])\n","torch.Size([16, 384, 128, 128])\n","torch.Size([16, 768, 64, 64])\n","torch.Size([16, 1536, 32, 32])\n"]}],"source":["print(image_feature['res2'].shape)\n","print(image_feature['res3'].shape)\n","print(image_feature['res4'].shape)\n","print(image_feature['res5'].shape)"]},{"cell_type":"markdown","metadata":{},"source":["## pixel_decoder"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["{'res2': ShapeSpec(channels=192, height=None, width=None, stride=4),\n"," 'res3': ShapeSpec(channels=384, height=None, width=None, stride=8),\n"," 'res4': ShapeSpec(channels=768, height=None, width=None, stride=16),\n"," 'res5': ShapeSpec(channels=1536, height=None, width=None, stride=32)}"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["backbone.output_shape()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Calling forward() may cause unpredicted behavior of PixelDecoder module.\n"]}],"source":["# model.py\n","from MODULES.MaskFormer.modeling.pixel_decoder.pixel_decoder import TransformerEncoderPixelDecoder\n","\n","def build_pixel_decoder(cfg, input_shape):\n","    model = TransformerEncoderPixelDecoder(cfg, input_shape)\n","    return model\n","\n","pixel_decoder = build_pixel_decoder(cfg, backbone.output_shape())\n","pp_embeds, image_features = pixel_decoder(image_feature)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([16, 256, 256, 256])"]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["pp_embeds.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([16, 256, 32, 32])"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["image_features.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from detectron2.modeling import build_sem_seg_head\n","\n","# sem_seg_head = build_sem_seg_head(cfg, backbone.output_shape())\n","# -> SEM_SEG_HEADS_REGISTRY.get(name)(cfg, input_shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from mask2former.maskformer_model.modeling.pixel_decoder.fpn import build_pixel_decoder\n","\n","# build_pixel_decoder(cfg, input_shape)\n","# -> model = SEM_SEG_HEADS_REGISTRY.get(name)(cfg, input_shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from mask2former.modeling.pixel_decoder.fpn import TransformerEncoderPixelDecoder\n","\n","# pixel_decoder = TransformerEncoderPixelDecoder(cfg, input_shape=backbone.output_shape()) # maskformer_model.py\n","\n","# # mask_foremer_head.py\n","# pp_embeds, transformer_encoder_features, _ = pixel_decoder.forward_features(features)"]},{"cell_type":"markdown","metadata":{},"source":["## mask_predictor"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["'swin_large_patch4_window12_384_22k.pth'"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["cfg.MODEL.WEIGHTS"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["audiomodule_weight = {\n","            'vggish': 'MODULES/ckpt/vggish-10086976.pth',\n","            'pca': 'MODULES/ckpt/vggish_pca_params-970ea276.pth'\n","            }"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([4, 160320, 1])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["import librosa\n","\n","audio_list = []\n","for path in data['audio_path']:\n","    mixed_audio, sr = librosa.load(path, mono=True, sr=16000)\n","    audio_list.append(torch.from_numpy(mixed_audio).unsqueeze(1))\n","\n","mixed_audio = ImageList.from_tensors(audio_list).tensor\n","mixed_audio.shape"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["from MODULES.MaskFormer.modeling.transformer.mask_predictor import MaskPredictor\n","\n","mask_predictor = MaskPredictor(cfg, cfg.MODEL.SEM_SEG_HEAD.CONVS_DIM, audiomodule_weight) # mask_former_head.py 참고"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["device = 'cuda:2'"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["mask_predictor.to(torch.device(device))\n","image_features = pp_embeds.to(device)\n","pp_embeds = pp_embeds.to(device)\n","mixed_audio = mixed_audio.to(device)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 160.00 MiB (GPU 2; 10.75 GiB total capacity; 9.31 GiB already allocated; 143.56 MiB free; 9.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_2480422/669237444.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpp_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmixed_audio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/envs/GEA/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/workspace/GitHub/AVSL/MODULES/MaskFormer/modeling/transformer/mask_predictor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, pp_embeds, mixed_audio)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m############## audio module ##############\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;31m# audio separator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0msep_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mASP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_audio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;34m\"\"\"sp: (batch_size*(num_tokens+1), channel_num:1, 502, 1025)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mixed_audio_spec\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/envs/GEA/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/workspace/GitHub/AVSL/MODULES/AudioModule/LAAS.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, condition)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mx_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_block1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# x1_pool: (bs, 32, T / 2, F / 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mx_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_block2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# x2_pool: (bs, 64, T / 4, F / 4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mx_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_block3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# x3_pool: (bs, 128, T / 8, F / 8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mx_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_block4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# x4_pool: (bs, 256, T / 16, F / 16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/envs/GEA/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/workspace/GitHub/AVSL/MODULES/AudioModule/LAAS.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, condition)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0mencoder_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mencoder_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/envs/GEA/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/workspace/GitHub/AVSL/MODULES/AudioModule/LAAS.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, condition)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;31m# x = act(self.bn2(self.conv2(x)), self.activation) + c2[:, :, None]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;31m#####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;31m# c1_ = self.emb_conv1(condition)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/envs/GEA/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/envs/GEA/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/envs/GEA/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 2; 10.75 GiB total capacity; 9.31 GiB already allocated; 143.56 MiB free; 9.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["output = mask_predictor(image_features, pp_embeds, mixed_audio)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["dict_keys(['pp_embeds', 'audio_sep_tokens', 'mixed_audio_spec', 'sep_audio_wavs', 'sep_audio_specs', 'sep_audio_features', 'sep_audio_features_embeds', 'pred_masks'])"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["output.keys()"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["pp_embeds torch.Size([4, 256, 256, 256])\n","audio_sep_tokens torch.Size([4, 4, 128])\n","mixed_audio_spec torch.Size([20, 1, 502, 1025])\n","sep_audio_wavs (20, 160320, 1)\n","sep_audio_specs (20, 1, 502, 1025)\n","sep_audio_features torch.Size([4, 4, 128])\n","sep_audio_features_embeds torch.Size([4, 4, 256])\n","pred_masks torch.Size([4, 4, 256, 256])\n"]}],"source":["for k in output.keys():\n","    print(k, output[k].shape)"]},{"cell_type":"markdown","metadata":{},"source":["## TransformerDecoder 수정"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # mask_former_head.py\n","# if cfg.MODEL.MASK_FORMER.TRANSFORMER_IN_FEATURE == \"transformer_encoder\":\n","#     transformer_predictor_in_channels = cfg.MODEL.SEM_SEG_HEAD.CONVS_DIM\n","# elif cfg.MODEL.MASK_FORMER.TRANSFORMER_IN_FEATURE == \"pixel_embedding\":\n","#     transformer_predictor_in_channels = cfg.MODEL.SEM_SEG_HEAD.MASK_DIM\n","# elif cfg.MODEL.MASK_FORMER.TRANSFORMER_IN_FEATURE == \"multi_scale_pixel_decoder\":  # for maskformer2\n","#     transformer_predictor_in_channels = cfg.MODEL.SEM_SEG_HEAD.CONVS_DIM\n","# else:\n","#     transformer_predictor_in_channels = input_shape[cfg.MODEL.MASK_FORMER.TRANSFORMER_IN_FEATURE].channels\n","\n","# # \"transformer_predictor\": build_transformer_decoder(cfg, transformer_predictor_in_channels, mask_classification=True,)\n","# # -> TRANSFORMER_DECODER_REGISTRY.get(name)(cfg, in_channels, mask_classification)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from mask2former.modeling.transformer_decoder.maskformer_transformer_decoder import StandardTransformerDecoder\n","\n","# transformer_decoder = StandardTransformerDecoder(cfg, in_channels=transformer_predictor_in_channels, mask_classification=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# mask_former_head.py\n","# predictions = transformer_decoder(transformer_encoder_features, mask_features, mask=None) # mask에 뭐가 들어가야 하는지 모르겠다"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# predictions.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# predictions['pred_masks'].shape # scoremap\n","# # predictions['pred_masks']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# print(predictions['pred_masks'].min())\n","# print(predictions['pred_masks'].max())\n","# # 음수 값이 나오는 것을 방지하기 위해 sigmoid 함수 고려"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# maskformer_transformer_decoder_custom.py\n","from MaskFormer.modeling.transformer.transformer import Transformer\n","from MaskFormer.modeling.transformer.position_encoding import PositionEmbeddingSine\n","from torch import nn\n","\n","N_steps = cfg.MODEL.MASK_FORMER.HIDDEN_DIM // 2\n","pe_layer = PositionEmbeddingSine(N_steps, normalize=True)\n","\n","batch_size=cfg.SOLVER.IMS_PER_BATCH\n","\n","num_queries = cfg.MODEL.MASK_FORMER.NUM_OBJECT_QUERIES\n","transformer = Transformer(\n","            d_model=cfg.MODEL.MASK_FORMER.HIDDEN_DIM,\n","            dropout=cfg.MODEL.MASK_FORMER.DROPOUT,\n","            nhead=cfg.MODEL.MASK_FORMER.NHEADS,\n","            dim_feedforward=cfg.MODEL.MASK_FORMER.DIM_FEEDFORWARD,\n","            num_encoder_layers=cfg.MODEL.MASK_FORMER.ENC_LAYERS,\n","            num_decoder_layers=cfg.MODEL.MASK_FORMER.DEC_LAYERS,\n","            normalize_before=cfg.MODEL.MASK_FORMER.PRE_NORM,\n","            return_intermediate_dec=cfg.MODEL.MASK_FORMER.DEEP_SUPERVISION,\n","        )\n","hidden_dim = transformer.d_model\n","\n","\n","if cfg.MODEL.MASK_FORMER.TRANSFORMER_IN_FEATURE == \"transformer_encoder\":\n","    transformer_predictor_in_channels = cfg.MODEL.SEM_SEG_HEAD.CONVS_DIM\n","elif cfg.MODEL.MASK_FORMER.TRANSFORMER_IN_FEATURE == \"pixel_embedding\":\n","    transformer_predictor_in_channels = cfg.MODEL.SEM_SEG_HEAD.MASK_DIM"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import fvcore.nn.weight_init as weight_init\n","from detectron2.layers import Conv2d\n","from MaskFormer.modeling.transformer.transformer_predictor import MLP\n","\n","learnable_input_queries = nn.Embedding(num_queries, hidden_dim)\n","\n","in_channels = transformer_predictor_in_channels # mask_former_head.py\n","enforce_input_project = cfg.MODEL.MASK_FORMER.ENFORCE_INPUT_PROJ\n","deep_supervision = cfg.MODEL.MASK_FORMER.DEEP_SUPERVISION\n","mask_dim = cfg.MODEL.SEM_SEG_HEAD.MASK_DIM\n","\n","if in_channels != hidden_dim or enforce_input_project:\n","    input_proj = Conv2d(in_channels, hidden_dim, kernel_size=1)\n","    weight_init.c2_xavier_fill(input_proj)\n","else:\n","    input_proj = nn.Sequential()\n","aux_loss = deep_supervision\n","\n","mask_embed = MLP(hidden_dim, hidden_dim, mask_dim, 3).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([4, 128])"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["learnable_input_queries.weight.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x = image_features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pos = pe_layer(x)\n","\n","src = x\n","mask = None\n","hs, memory = transformer(input_proj(src), mask, learnable_input_queries.weight, pos)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# output_size = hs.shape\n","\n","audio_separation_tokens = hs[-1]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### LAAS"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/workspace/GitHub/AVSL/models\n"]}],"source":["%cd /workspace/GitHub/AVSL/models\n","# pytorch-lightning: pip install pytorch-lightning==1.4.4, pip install torchmetrics==0.6.0"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([16, 161760, 1])"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["import librosa\n","\n","audio_list = []\n","for path in data['audio_path']:\n","    y, sr = librosa.load(path, mono=True, sr=16000)\n","    audio_list.append(torch.from_numpy(y).unsqueeze(1))\n","\n","y = ImageList.from_tensors(audio_list).tensor\n","y.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([16, 4, 128])"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["audio_separation_tokens.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 128])\n","torch.Size([16, 1, 128])\n"]}],"source":["noise_token = nn.Embedding(1, hidden_dim)\n","print(noise_token.weight.shape)\n","noise_token = noise_token.weight.unsqueeze(0).repeat(batch_size, 1, 1)\n","print(noise_token.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([16, 5, 128])"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["input_tokens = torch.cat([audio_separation_tokens, noise_token], dim=1)\n","input_tokens.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# x = {'audio':torch.from_numpy(y.reshape((2,y.shape[1],1))),\n","#      'conditions':torch.from_numpy(np.zeros((2,9,128),dtype=np.float32))} # (batch_size, 오디오길이, 1), mono로 바꾸어야 함\n","\n","input = {\"audio\": y, \"conditions\": input_tokens}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from AudioModule.LAAS import LAAS\n","from AudioModule import config\n","\n","laas = LAAS(config=config)\n","audio_out = laas(input)\n","# audio_out = laas(input).to(device) # [batch_size*N, audio_len, hidden_dim]\n","# batch_size = 16, N = 4 일때 1분 20초"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([64, 5, 128])\n","torch.Size([80, 1, 503, 1025])\n"]}],"source":["print(audio_out[\"separated_audio_features\"].shape)\n","print(audio_out[\"mixed_audio\"].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["separated_audio_features = audio_out[\"separated_audio_features\"]\n","separated_audio_features = separated_audio_features.unsqueeze(1)\n","size = separated_audio_features.shape\n","separated_audio_features = separated_audio_features.reshape(16, 4, size[2], size[3]) # torch.Size([16, 4, 5, 128])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import torch\n","\n","# y = torch.rand(4, 161120, 1)\n","# audio_separation_tokens = torch.rand(4, 4, 128)\n","# token_size = audio_separation_tokens.shape\n","# noise_token = torch.rand(token_size[0], 1, token_size[2])\n","# input_tokens = torch.cat([audio_separation_tokens, noise_token], dim=1)\n","\n","# input = {\"audio\": y, \"conditions\": input_tokens}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# device = \"cuda:6\"\n","# input = {\"audio\": y.to(device), \"conditions\": input_tokens.to(device)}\n","\n","# ASP.to(device)\n","# x = ASP(input=input[\"audio\"], condition=input[\"conditions\"])\n","\n","# from AudioModule.LAAS import LAAS\n","# from AudioModule import config\n","\n","# laas = LAAS(config=config)\n","# laas.to(device)\n","# audio_features = laas(input).to(device)"]},{"cell_type":"markdown","metadata":{},"source":["### TransformerPredictor"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([16, 4, 5, 128])"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["separated_audio_features.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([16, 4, 128])"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["def av_feature_embed(separated_audio_features, pooling_mode='sum'):\n","    # av_feature_list = ImageList.from_tensors(separated_audio_features).tensor\n","    # av_feature_list = av_feature_list.permute(1, 0, 2, 3)\n","    if pooling_mode == 'sum':\n","        separated_audio_features = separated_audio_features.sum(dim=2)\n","    elif pooling_mode == 'mean':\n","        separated_audio_features = separated_audio_features.mean(dim=2)\n","    elif pooling_mode == 'max':\n","        separated_audio_features = separated_audio_features.max(dim=2).values\n","    return separated_audio_features\n","\n","separated_audio_features = av_feature_embed(separated_audio_features, pooling_mode='sum')\n","separated_audio_features.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([16, 4, 256])"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["separated_audio_features_embed = mask_embed(separated_audio_features)\n","separated_audio_features_embed.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pp_embeds = pp_embeds.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred_masks = torch.einsum(\"bqc,bchw->bqhw\", separated_audio_features_embed, pp_embeds)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([16, 4, 256, 256])"]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["pred_masks.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# def av_feature_embed(separated_audio_wav_features_list, pooling_mode='sum'):\n","#     av_feature_list = ImageList.from_tensors(separated_audio_wav_features_list).tensor\n","#     av_feature_list = av_feature_list.permute(1, 0, 2, 3)\n","#     if pooling_mode == 'sum':\n","#         av_feature_list = av_feature_list.sum(dim=2)\n","#     elif pooling_mode == 'mean':\n","#         av_feature_list = av_feature_list.mean(dim=2)\n","#     elif pooling_mode == 'max':\n","#         av_feature_list = av_feature_list.max(dim=2).values\n","#     return av_feature_list\n","\n","# av_feature_list = av_feature_embed(separated_audio_wav_features_list, pooling_mode='sum')\n","# av_feature_list.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# av_feature_embed = mask_embed(av_feature_list)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# av_feature_embed.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# scoremap = torch.einsum(\"bqc,bchw->bqhw\", av_feature_embed, mask_features)\n","# scoremap.shape\n","# scoremap = torch.einsum(\"lbqc,bchw->lbqhw\", av_feature_embed, mask_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1706449382705,"user":{"displayName":"구은아","userId":"03420424854508517452"},"user_tz":-540},"id":"9i6cJdI2D2AN","outputId":"b7fe9b2f-a8bd-4f2f-be59-567130d04da0"},"outputs":[],"source":["# from mask2former.modeling.transformer_decoder.maskformer_transformer_decoder_custom import StandardTransformerDecoder\n","\n","# def build_transformer_decoder(cfg):\n","#     if cfg.MODEL.MASK_FORMER.TRANSFORMER_IN_FEATURE == \"transformer_encoder\":\n","#         transformer_predictor_in_channels = cfg.MODEL.SEM_SEG_HEAD.CONVS_DIM\n","#     elif cfg.MODEL.MASK_FORMER.TRANSFORMER_IN_FEATURE == \"pixel_embedding\":\n","#         transformer_predictor_in_channels = cfg.MODEL.SEM_SEG_HEAD.MASK_DIM\n","#     elif cfg.MODEL.MASK_FORMER.TRANSFORMER_IN_FEATURE == \"multi_scale_pixel_decoder\":  # for maskformer2\n","#         transformer_predictor_in_channels = cfg.MODEL.SEM_SEG_HEAD.CONVS_DIM\n","#     model = StandardTransformerDecoder(cfg, in_channels=transformer_predictor_in_channels, mask_classification=False)\n","#     return model\n","\n","# transformer_decoder = build_transformer_decoder(cfg)\n","# outputs = transformer_decoder(transformer_encoder_features, mask_features, mixed_audio=None, mask=None)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# outputs.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2599,"status":"ok","timestamp":1706449387852,"user":{"displayName":"구은아","userId":"03420424854508517452"},"user_tz":-540},"id":"xBUymms27gMA"},"outputs":[{"ename":"NameError","evalue":"name 'mask_pred_results' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_1098973/99983350.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m mask_pred_results = F.interpolate(\n\u001b[0;32m----> 2\u001b[0;31m                 \u001b[0mmask_pred_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m                 \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bilinear\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0malign_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'mask_pred_results' is not defined"]}],"source":["mask_pred_results = F.interpolate(\n","                mask_pred_results,\n","                size=(images.tensor.shape[-2], images.tensor.shape[-1]),\n","                mode=\"bilinear\",\n","                align_corners=False,\n","            )"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1706449430235,"user":{"displayName":"구은아","userId":"03420424854508517452"},"user_tz":-540},"id":"s__BZNbIT5i0"},"outputs":[],"source":["image_size = images.image_sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":371,"status":"ok","timestamp":1706450905743,"user":{"displayName":"구은아","userId":"03420424854508517452"},"user_tz":-540},"id":"s9AH6HgVVBNT"},"outputs":[],"source":["def semantic_inference(mask_cls, mask_pred):\n","    mask_cls = F.softmax(mask_cls, dim=-1)[..., :-1]\n","    mask_pred = mask_pred.sigmoid()\n","    semseg = torch.einsum(\"qc,qhw->chw\", mask_cls, mask_pred)\n","    return semseg\n","\n","def panoptic_inference(mask_cls, mask_pred):\n","    scores, labels = F.softmax(mask_cls, dim=-1).max(-1)\n","    mask_pred = mask_pred.sigmoid()\n","\n","    keep = labels.ne(sem_seg_head.num_classes) & (scores > object_mask_threshold)\n","    cur_scores = scores[keep]\n","    cur_classes = labels[keep]\n","    cur_masks = mask_pred[keep]\n","    cur_mask_cls = mask_cls[keep]\n","    cur_mask_cls = cur_mask_cls[:, :-1]\n","\n","    cur_prob_masks = cur_scores.view(-1, 1, 1) * cur_masks\n","\n","    h, w = cur_masks.shape[-2:]\n","    panoptic_seg = torch.zeros((h, w), dtype=torch.int32, device=cur_masks.device)\n","    segments_info = []\n","\n","    current_segment_id = 0\n","\n","    if cur_masks.shape[0] == 0:\n","        # We didn't detect any mask :(\n","        return panoptic_seg, segments_info\n","    else:\n","        # take argmax\n","        cur_mask_ids = cur_prob_masks.argmax(0)\n","        stuff_memory_list = {}\n","        for k in range(cur_classes.shape[0]):\n","            pred_class = cur_classes[k].item()\n","            isthing = pred_class in metadata.thing_dataset_id_to_contiguous_id.values()\n","            mask_area = (cur_mask_ids == k).sum().item()\n","            original_area = (cur_masks[k] >= 0.5).sum().item()\n","            mask = (cur_mask_ids == k) & (cur_masks[k] >= 0.5)\n","\n","            if mask_area > 0 and original_area > 0 and mask.sum().item() > 0:\n","                if mask_area / original_area < overlap_threshold:\n","                    continue\n","\n","                # merge stuff regions\n","                if not isthing:\n","                    if int(pred_class) in stuff_memory_list.keys():\n","                        panoptic_seg[mask] = stuff_memory_list[int(pred_class)]\n","                        continue\n","                    else:\n","                        stuff_memory_list[int(pred_class)] = current_segment_id + 1\n","\n","                current_segment_id += 1\n","                panoptic_seg[mask] = current_segment_id\n","\n","                segments_info.append(\n","                    {\n","                        \"id\": current_segment_id,\n","                        \"isthing\": bool(isthing),\n","                        \"category_id\": int(pred_class),\n","                    }\n","                )\n","\n","        return panoptic_seg, segments_info\n","\n","def instance_inference(mask_cls, mask_pred):\n","    # mask_pred is already processed to have the same shape as original input\n","    image_size = mask_pred.shape[-2:]\n","\n","    # [Q, K]\n","    scores = F.softmax(mask_cls, dim=-1)[:, :-1]\n","    labels = torch.arange(sem_seg_head.num_classes, device=device).unsqueeze(0).repeat(num_queries, 1).flatten(0, 1)\n","    # scores_per_image, topk_indices = scores.flatten(0, 1).topk(num_queries, sorted=False)\n","    scores_per_image, topk_indices = scores.flatten(0, 1).topk(test_topk_per_image, sorted=False)\n","    labels_per_image = labels[topk_indices]\n","\n","    topk_indices = topk_indices // sem_seg_head.num_classes\n","    # mask_pred = mask_pred.unsqueeze(1).repeat(1, sem_seg_head.num_classes, 1).flatten(0, 1)\n","    mask_pred = mask_pred[topk_indices]\n","\n","    # if this is panoptic segmentation, we only keep the \"thing\" classes\n","    if panoptic_on:\n","        keep = torch.zeros_like(scores_per_image).bool()\n","        for i, lab in enumerate(labels_per_image):\n","            keep[i] = lab in metadata.thing_dataset_id_to_contiguous_id.values()\n","\n","        scores_per_image = scores_per_image[keep]\n","        labels_per_image = labels_per_image[keep]\n","        mask_pred = mask_pred[keep]\n","\n","    result = Instances(image_size)\n","    # mask (before sigmoid)\n","    result.pred_masks = (mask_pred > 0).float()\n","    result.pred_boxes = Boxes(torch.zeros(mask_pred.size(0), 4))\n","    # Uncomment the following to get boxes from masks (this is slow)\n","    # result.pred_boxes = BitMasks(mask_pred > 0).get_bounding_boxes()\n","\n","    # calculate average mask prob\n","    mask_scores_per_image = (mask_pred.sigmoid().flatten(1) * result.pred_masks.flatten(1)).sum(1) / (result.pred_masks.flatten(1).sum(1) + 1e-6)\n","    result.scores = scores_per_image * mask_scores_per_image\n","    result.pred_classes = labels_per_image\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8557,"status":"ok","timestamp":1706450933103,"user":{"displayName":"구은아","userId":"03420424854508517452"},"user_tz":-540},"id":"HENIYgLYTzxi"},"outputs":[],"source":["processed_results = []\n","batched_inputs = dataset_dict[\"image\"]\n","\n","for mask_cls_result, mask_pred_result, input_per_image, image_size in zip(\n","    mask_cls_results, mask_pred_results, batched_inputs, images.image_sizes):\n","    height = 1024 # input_per_image.get(\"height\", image_size[0])\n","    width = 1024 # input_per_image.get(\"width\", image_size[1])\n","    processed_results.append({})\n","\n","    if sem_seg_postprocess_before_inference:\n","        mask_pred_result = retry_if_cuda_oom(sem_seg_postprocess)(\n","            mask_pred_result, image_size, height, width\n","        )\n","        mask_cls_result = mask_cls_result.to(mask_pred_result)\n","\n","    # semantic segmentation inference\n","    if semantic_on:\n","        r = retry_if_cuda_oom(semantic_inference)(mask_cls_result, mask_pred_result)\n","        if not sem_seg_postprocess_before_inference:\n","            r = retry_if_cuda_oom(sem_seg_postprocess)(r, image_size, height, width)\n","        processed_results[-1][\"sem_seg\"] = r\n","\n","    # panoptic segmentation inference\n","    if panoptic_on:\n","        panoptic_r = retry_if_cuda_oom(panoptic_inference)(mask_cls_result, mask_pred_result)\n","        processed_results[-1][\"panoptic_seg\"] = panoptic_r\n","\n","    # instance segmentation inference\n","    if instance_on:\n","        instance_r = retry_if_cuda_oom(instance_inference)(mask_cls_result, mask_pred_result)\n","        processed_results[-1][\"instances\"] = instance_r\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1706451124981,"user":{"displayName":"구은아","userId":"03420424854508517452"},"user_tz":-540},"id":"9rbBuF-4TzuD","outputId":"afcdb8f5-41e4-4c5f-cac9-8fb686e58f59"},"outputs":[{"data":{"text/plain":["dict_keys(['sem_seg', 'panoptic_seg', 'instances'])"]},"execution_count":72,"metadata":{},"output_type":"execute_result"}],"source":["processed_results[0].keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1706451149416,"user":{"displayName":"구은아","userId":"03420424854508517452"},"user_tz":-540},"id":"WnJ-7xzQTzqb","outputId":"92856944-a6ca-4979-eea0-e98dbc5fc447"},"outputs":[{"data":{"text/plain":["torch.Size([133, 1024, 1024])"]},"execution_count":74,"metadata":{},"output_type":"execute_result"}],"source":["processed_results[0]['sem_seg'].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-EJulqDDTzGa"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["pDe-87-15hMh"],"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"}},"nbformat":4,"nbformat_minor":0}
